2025-05-16 12:42:48,548 - app - INFO - User message: Hello
2025-05-16 12:42:48,549 - app - INFO - Bot response: Hello! I'm your MCP Chatbot. I can help with simple calculations. Try asking me to add two numbers!
2025-05-16 12:42:48,549 - app - INFO - User message: Hi again
2025-05-16 12:42:48,550 - app - INFO - Bot response: Hello! I'm your MCP Chatbot. I can help with simple calculations. Try asking me to add two numbers!
2025-05-16 12:42:48,551 - app - INFO - User message: Another message
2025-05-16 12:42:48,551 - app - INFO - Bot response: I'm a simple chatbot that can help with basic math. Try asking me to add two numbers!
2025-05-16 13:06:39,629 - app - INFO - Switched to local backend
2025-05-16 13:06:39,630 - app - INFO - User message: Hi
2025-05-16 13:06:39,630 - app - INFO - Bot response: Hello! I'm your MCP Chatbot. I can help with simple calculations. Try asking me to add two numbers!
2025-05-16 13:06:39,630 - app - INFO - Switched to openai backend
2025-05-16 13:06:39,631 - app - INFO - User message: Hi
2025-05-16 13:06:41,655 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 13:06:41,661 - app - INFO - Bot response: Hello! How can I assist you today?
2025-05-16 13:17:03,920 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 13:17:03,920 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 13:17:03,920 - __main__ - INFO - Visit http://0.0.0.0:7860 to chat
2025-05-16 13:17:03,920 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 13:17:05,991 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-16 13:17:06,724 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 13:17:06,728 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 13:20:35,637 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 13:20:35,637 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 13:20:35,637 - __main__ - INFO - Visit http://0.0.0.0:7860 to chat
2025-05-16 13:20:35,639 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 13:20:37,686 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-16 13:20:38,440 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 13:20:38,441 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 13:27:20,672 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 13:27:20,672 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 13:27:20,672 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 13:27:20,672 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 13:27:22,748 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-16 13:27:23,444 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 13:27:23,444 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 13:27:25,346 - httpx - INFO - HTTP Request: GET http://localhost:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-16 13:27:27,415 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 13:29:16,951 - __main__ - INFO - User message: ok test
2025-05-16 13:29:16,952 - __main__ - INFO - Bot response: I'm a simple chatbot that can help with basic math. Try asking me to add two numbers!
2025-05-16 13:29:21,398 - __main__ - INFO - Switched to openai backend
2025-05-16 13:29:26,976 - __main__ - INFO - User message: what are boyu are you good?
2025-05-16 13:29:27,616 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 13:29:27,621 - __main__ - INFO - Bot response: I'm a computer program designed to assist with basic tasks and provide information. How can I help you today?
2025-05-16 13:51:46,853 - __main__ - INFO - User message: hello
2025-05-16 13:51:47,672 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 13:51:47,673 - __main__ - INFO - Bot response: Hello! How can I assist you today?
2025-05-16 13:51:51,766 - __main__ - INFO - User message: what are you good at?
2025-05-16 13:51:52,472 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 13:51:52,479 - __main__ - INFO - Bot response: I am good at providing information, answering questions, offering suggestions, and helping with a wide range of topics. Feel free to ask me anything you'd like assistance with!
2025-05-16 13:51:55,901 - __main__ - INFO - Switched to openai backend
2025-05-16 13:52:11,284 - __main__ - INFO - User message: what are you good at? and what is your model? and what is the hardest prompt you encountered?
2025-05-16 13:52:12,595 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 13:52:12,598 - __main__ - INFO - Bot response: I am a language model AI trained to assist users with a variety of tasks such as answering questions, providing information, and engaging in conversations. My model is based on cutting-edge natural language processing technology.

As for the hardest prompt I have encountered, it's difficult to pinpoint a specific example as my training data covers a broad range of topics and scenarios. However, I am continually learning and improving to better assist users with their queries. Feel free to ask me anything you'd like help with!
2025-05-16 13:52:37,053 - __main__ - INFO - Switched to local backend
2025-05-16 13:52:40,719 - __main__ - INFO - User message: and you?
2025-05-16 13:52:40,721 - __main__ - INFO - Bot response: I'm a simple chatbot that can help with basic math. Try asking me to add two numbers!
2025-05-16 14:05:33,255 - __main__ - WARNING - Google Generative AI package not installed. Install with: pip install google-generativeai
2025-05-16 14:05:33,255 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:05:33,256 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:05:33,256 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:05:33,256 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:05:37,369 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000275923E58B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:05:37,370 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:05:37,906 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:05:37,944 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:05:38,202 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:05:38,239 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:05:38,760 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:05:38,774 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:05:40,034 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:05:42,243 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:05:43,016 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ "HTTP/1.1 200 OK"
2025-05-16 14:06:58,828 - __main__ - WARNING - Google Generative AI package not installed. Install with: pip install google-generativeai
2025-05-16 14:06:58,828 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:06:58,828 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:06:58,829 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:06:58,829 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:07:02,919 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020A012617C0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:07:02,919 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:07:03,371 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:07:03,385 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:07:03,627 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:07:03,700 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:07:04,075 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:07:04,102 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:07:05,425 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:07:07,494 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:07:08,182 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ "HTTP/1.1 200 OK"
2025-05-16 14:08:57,373 - __main__ - WARNING - Google Generative AI package not installed. Install with: pip install google-generativeai
2025-05-16 14:08:57,373 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:08:57,374 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:08:57,374 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:08:57,374 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:08:57,376 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:09:01,469 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B1885AE480>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:09:01,469 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:09:01,473 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:09:01,609 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:09:01,817 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:09:01,823 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:09:01,843 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1888FEA50>
2025-05-16 14:09:01,844 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B18860EDD0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:09:01,867 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1887AA180>
2025-05-16 14:09:01,867 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:09:01,867 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:01,868 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:09:01,869 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:01,869 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:09:01,871 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:09:01,872 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:09:01,893 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:09:02 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:09:01,893 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:09:01,893 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1888AC1D0>
2025-05-16 14:09:01,894 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:09:01,894 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B1886CDE50> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:09:01,894 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:01,894 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:01,894 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:01,894 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:01,894 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:01,920 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1888FFFB0>
2025-05-16 14:09:01,921 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:09:01,922 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:01,922 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:09:01,922 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:01,922 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:09:01,935 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1888FF1D0>
2025-05-16 14:09:01,935 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B18860F4D0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:09:01,938 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:09:01,941 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:09:02 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:09:01,941 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:09:01,941 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:09:01,941 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:01,941 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:01,941 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:01,941 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:01,941 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:01,968 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1888FF9E0>
2025-05-16 14:09:01,969 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B1886CDFD0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:09:02,131 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None
2025-05-16 14:09:02,131 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1888FF0E0>
2025-05-16 14:09:02,132 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:09:02,132 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:02,132 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:09:02,133 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:02,133 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:09:02,169 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None
2025-05-16 14:09:02,170 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1888FF8F0>
2025-05-16 14:09:02,171 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:09:02,171 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:02,172 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:09:02,172 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:02,172 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:09:02,222 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B188921A00>
2025-05-16 14:09:02,222 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B18860C6D0> server_hostname='api.gradio.app' timeout=5
2025-05-16 14:09:02,224 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:09:02 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:09:02,224 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:09:02,224 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:09:02,226 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:02,226 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:02,226 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:02,226 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:02,226 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:02,266 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1884BB770>
2025-05-16 14:09:02,267 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B1886BE9D0> server_hostname='api.gradio.app' timeout=5
2025-05-16 14:09:02,267 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:09:02 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:09:02,267 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:09:02,267 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:09:02,267 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:02,267 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:02,267 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:02,268 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:02,268 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:02,426 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B188646870>
2025-05-16 14:09:02,427 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:09:02,427 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:02,428 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:09:02,428 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:02,428 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:09:02,463 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B188411040>
2025-05-16 14:09:02,464 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:09:02,465 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:02,465 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:09:02,465 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:02,465 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:09:02,611 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:09:02 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])
2025-05-16 14:09:02,612 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:09:02,612 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:09:02,612 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:02,612 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:02,612 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:02,612 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:02,613 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:02,649 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:09:02 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])
2025-05-16 14:09:02,649 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:09:02,650 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:09:02,650 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:02,650 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:02,650 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:02,651 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:02,651 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:04,000 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1886C9EB0>
2025-05-16 14:09:04,000 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:09:04,001 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:04,001 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:09:04,001 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:04,001 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:09:04,001 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:09:03 GMT'), (b'server', b'uvicorn'), (b'content-length', b'5'), (b'content-type', b'application/json')])
2025-05-16 14:09:04,001 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:09:04,002 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:09:04,002 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:04,002 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:04,002 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:04,003 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:04,003 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:04,011 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=3 socket_options=None
2025-05-16 14:09:06,083 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B1889230E0>
2025-05-16 14:09:06,083 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-05-16 14:09:06,083 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:06,083 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-05-16 14:09:06,083 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:06,084 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-05-16 14:09:06,097 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:09:06 GMT'), (b'server', b'uvicorn'), (b'content-length', b'13833'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-16 14:09:06,098 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:09:06,098 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-05-16 14:09:06,098 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:06,098 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:06,098 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:06,098 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:06,098 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:06,280 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None
2025-05-16 14:09:06,374 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B188921070>
2025-05-16 14:09:06,375 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B1888FAED0> server_hostname='api.gradio.app' timeout=5
2025-05-16 14:09:06,563 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B188921040>
2025-05-16 14:09:06,563 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:09:06,563 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:06,564 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:09:06,564 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:06,564 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:09:06,741 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:09:06 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])
2025-05-16 14:09:06,742 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ "HTTP/1.1 200 OK"
2025-05-16 14:09:06,742 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:09:06,742 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:06,742 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:06,743 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:06,743 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:06,743 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:35,655 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:09:35,656 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:09:35,657 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:09:35,657 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:09:35,658 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:09:39,765 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001675B7A40B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:09:39,765 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:09:39,768 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:09:39,900 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:09:40,089 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:09:40,093 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:09:40,130 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:09:40,130 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD58EC0>
2025-05-16 14:09:40,131 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001675B779D50> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:09:40,132 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:09:40,158 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD5A330>
2025-05-16 14:09:40,160 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001675B855F50> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:09:40,160 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD58DD0>
2025-05-16 14:09:40,161 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:09:40,161 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:40,161 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:09:40,161 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:40,162 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:09:40,185 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:09:40,191 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:09:40 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:09:40,191 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:09:40,192 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD20D40>
2025-05-16 14:09:40,192 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:09:40,192 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:09:40,192 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:40,193 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:40,193 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:40,193 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:40,193 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:09:40,194 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:40,194 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:40,194 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:09:40,194 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:40,213 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675B8B2A80>
2025-05-16 14:09:40,214 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:09:40 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:09:40,214 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001675B779750> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:09:40,214 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:09:40,214 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:09:40,214 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:40,215 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:40,215 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:40,215 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:40,215 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:40,225 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD59CA0>
2025-05-16 14:09:40,226 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001675B8560D0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:09:40,402 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD594F0>
2025-05-16 14:09:40,403 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:09:40,403 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:40,403 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:09:40,403 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:40,403 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:09:40,412 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None
2025-05-16 14:09:40,415 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD59BB0>
2025-05-16 14:09:40,416 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:09:40,416 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:40,416 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:09:40,416 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:40,416 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:09:40,435 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None
2025-05-16 14:09:40,496 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:09:40 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:09:40,497 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:09:40,497 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:09:40,497 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:40,498 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:40,498 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:40,498 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:40,499 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:40,518 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:09:40 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:09:40,518 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD5BD70>
2025-05-16 14:09:40,518 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:09:40,518 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001675B6796D0> server_hostname='api.gradio.app' timeout=5
2025-05-16 14:09:40,520 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:09:40,520 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:40,520 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:40,520 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:40,521 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:40,521 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:40,530 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675B7E52E0>
2025-05-16 14:09:40,530 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001675B855CD0> server_hostname='api.gradio.app' timeout=5
2025-05-16 14:09:40,710 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675B83AAB0>
2025-05-16 14:09:40,710 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:09:40,711 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:40,711 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:09:40,712 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:40,712 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:09:40,722 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675B7E67E0>
2025-05-16 14:09:40,722 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:09:40,723 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:40,723 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:09:40,724 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:40,724 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:09:40,888 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:09:40 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])
2025-05-16 14:09:40,889 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:09:40,890 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:09:40,890 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:40,891 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:40,891 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:40,891 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:40,892 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:40,911 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:09:40 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])
2025-05-16 14:09:40,912 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:09:40,912 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:09:40,912 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:40,912 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:40,912 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:40,912 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:40,912 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:42,235 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675588C7A0>
2025-05-16 14:09:42,235 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:09:42,236 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:42,236 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:09:42,237 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:42,237 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:09:42,237 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:09:42 GMT'), (b'server', b'uvicorn'), (b'content-length', b'5'), (b'content-type', b'application/json')])
2025-05-16 14:09:42,237 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:09:42,238 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:09:42,238 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:42,238 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:42,238 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:42,238 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:42,239 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:42,246 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=3 socket_options=None
2025-05-16 14:09:44,277 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD69250>
2025-05-16 14:09:44,278 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-05-16 14:09:44,279 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:44,279 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-05-16 14:09:44,279 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:44,280 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-05-16 14:09:44,292 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:09:43 GMT'), (b'server', b'uvicorn'), (b'content-length', b'13834'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-16 14:09:44,293 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:09:44,294 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-05-16 14:09:44,294 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:44,294 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:44,295 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:44,296 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:44,296 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:09:44,541 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None
2025-05-16 14:09:44,636 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD5A600>
2025-05-16 14:09:44,637 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001675CD4D450> server_hostname='api.gradio.app' timeout=5
2025-05-16 14:09:44,829 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001675CD5A180>
2025-05-16 14:09:44,829 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:09:44,830 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:09:44,830 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:09:44,830 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:09:44,830 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:09:45,006 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:09:45 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])
2025-05-16 14:09:45,007 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ "HTTP/1.1 200 OK"
2025-05-16 14:09:45,007 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:09:45,007 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:09:45,008 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:09:45,008 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:09:45,008 - httpcore.connection - DEBUG - close.started
2025-05-16 14:09:45,009 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:10:26,495 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:10:26,496 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:10:26,496 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:10:26,496 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:10:26,497 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:10:30,616 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001FA84AD6B40>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:10:30,616 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:10:30,619 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:10:30,778 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:10:30,939 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:10:30,962 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:10:30,978 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA861C8E60>
2025-05-16 14:10:30,978 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001FA84AD99D0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:10:31,002 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA85153710>
2025-05-16 14:10:31,002 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:10:31,003 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:10:31,003 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:10:31,003 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:10:31,003 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:10:31,003 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:10:31,004 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:10:31,022 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:10:31 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:10:31,022 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:10:31,022 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:10:31,022 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:10:31,023 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:10:31,023 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:10:31,023 - httpcore.connection - DEBUG - close.started
2025-05-16 14:10:31,024 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:10:31,024 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA8497A060>
2025-05-16 14:10:31,024 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001FA84BB6050> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:10:31,047 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA861CA450>
2025-05-16 14:10:31,047 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:10:31,047 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:10:31,048 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:10:31,048 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:10:31,048 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:10:31,060 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA861C95B0>
2025-05-16 14:10:31,060 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001FA84AD9750> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:10:31,067 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:10:31 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:10:31,067 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:10:31,068 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:10:31,068 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:10:31,068 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:10:31,069 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:10:31,069 - httpcore.connection - DEBUG - close.started
2025-05-16 14:10:31,069 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:10:31,084 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:10:31,091 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA861C9F40>
2025-05-16 14:10:31,092 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001FA84BB60D0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:10:31,253 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA861C94C0>
2025-05-16 14:10:31,253 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:10:31,254 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:10:31,255 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:10:31,255 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:10:31,255 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:10:31,259 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None
2025-05-16 14:10:31,280 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA861C9E50>
2025-05-16 14:10:31,281 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:10:31,281 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:10:31,282 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:10:31,282 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:10:31,282 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:10:31,299 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None
2025-05-16 14:10:31,350 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:10:31 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:10:31,350 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:10:31,351 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:10:31,351 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:10:31,351 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:10:31,351 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:10:31,352 - httpcore.connection - DEBUG - close.started
2025-05-16 14:10:31,352 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:10:31,353 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA84A246B0>
2025-05-16 14:10:31,353 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001FA849D96D0> server_hostname='api.gradio.app' timeout=5
2025-05-16 14:10:31,376 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:10:31 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:10:31,376 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:10:31,377 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:10:31,377 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:10:31,377 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:10:31,377 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:10:31,377 - httpcore.connection - DEBUG - close.started
2025-05-16 14:10:31,377 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:10:31,395 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA84B3E270>
2025-05-16 14:10:31,395 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001FA84BB5CD0> server_hostname='api.gradio.app' timeout=5
2025-05-16 14:10:31,539 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA837094F0>
2025-05-16 14:10:31,539 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:10:31,539 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:10:31,540 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:10:31,540 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:10:31,540 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:10:31,585 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA84B00BC0>
2025-05-16 14:10:31,585 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:10:31,586 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:10:31,586 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:10:31,586 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:10:31,586 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:10:31,722 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:10:31 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])
2025-05-16 14:10:31,722 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:10:31,723 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:10:31,723 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:10:31,723 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:10:31,723 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:10:31,723 - httpcore.connection - DEBUG - close.started
2025-05-16 14:10:31,723 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:10:32,189 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:10:32 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])
2025-05-16 14:10:32,189 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:10:32,190 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:10:32,190 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:10:32,190 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:10:32,190 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:10:32,190 - httpcore.connection - DEBUG - close.started
2025-05-16 14:10:32,191 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:10:33,169 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA84BC4F80>
2025-05-16 14:10:33,169 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:10:33,169 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:10:33,169 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:10:33,170 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:10:33,170 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:10:33,170 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:10:32 GMT'), (b'server', b'uvicorn'), (b'content-length', b'5'), (b'content-type', b'application/json')])
2025-05-16 14:10:33,170 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:10:33,170 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:10:33,170 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:10:33,172 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:10:33,172 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:10:33,172 - httpcore.connection - DEBUG - close.started
2025-05-16 14:10:33,172 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:10:33,179 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=3 socket_options=None
2025-05-16 14:10:35,252 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA861D93A0>
2025-05-16 14:10:35,253 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-05-16 14:10:35,255 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:10:35,256 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-05-16 14:10:35,256 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:10:35,257 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-05-16 14:10:35,272 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:10:35 GMT'), (b'server', b'uvicorn'), (b'content-length', b'13832'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-16 14:10:35,273 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:10:35,273 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-05-16 14:10:35,273 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:10:35,273 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:10:35,273 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:10:35,274 - httpcore.connection - DEBUG - close.started
2025-05-16 14:10:35,274 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:10:35,446 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None
2025-05-16 14:10:35,539 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA861CA180>
2025-05-16 14:10:35,540 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001FA861BDA50> server_hostname='api.gradio.app' timeout=5
2025-05-16 14:10:35,739 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001FA861CAE40>
2025-05-16 14:10:35,739 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:10:35,740 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:10:35,740 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:10:35,740 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:10:35,740 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:10:35,920 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:10:36 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])
2025-05-16 14:10:35,920 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ "HTTP/1.1 200 OK"
2025-05-16 14:10:35,922 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:10:35,922 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:10:35,922 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:10:35,922 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:10:35,922 - httpcore.connection - DEBUG - close.started
2025-05-16 14:10:35,922 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:12:22,890 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:12:22,891 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:12:22,891 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:12:22,891 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:12:26,999 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000245C4D252B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:12:26,999 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:12:27,448 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:12:27,449 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:12:27,729 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:12:27,757 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:12:28,174 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:12:28,181 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-16 14:12:29,518 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:12:31,585 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:12:32,238 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ "HTTP/1.1 200 OK"
2025-05-16 14:15:10,257 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:15:10,257 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:15:10,257 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:15:10,257 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:15:10,258 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:15:14,371 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002675D529E80>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:15:14,372 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:15:14,375 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:15:14,576 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:15:14,698 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:15:14,721 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:15:14,748 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:15:14,748 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:15:14,758 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675E954D70>
2025-05-16 14:15:14,759 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002675D5273D0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:15:14,784 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675E881730>
2025-05-16 14:15:14,785 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:15:14,785 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:15:14,786 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:15:14,786 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:15:14,786 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:15:14,790 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675E989B50>
2025-05-16 14:15:14,791 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002675D674A50> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:15:14,806 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:15:14 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:15:14,806 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:15:14,806 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:15:14,807 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:15:14,807 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:15:14,807 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:15:14,807 - httpcore.connection - DEBUG - close.started
2025-05-16 14:15:14,807 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:15:14,809 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:15:14,811 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675E989A60>
2025-05-16 14:15:14,811 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:15:14,813 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:15:14,813 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:15:14,813 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675E988D40>
2025-05-16 14:15:14,813 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:15:14,813 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002675D527750> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:15:14,815 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:15:14,836 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:15:14 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:15:14,837 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:15:14,837 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:15:14,837 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:15:14,837 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:15:14,837 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:15:14,837 - httpcore.connection - DEBUG - close.started
2025-05-16 14:15:14,837 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:15:14,838 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:15:14,846 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=None socket_options=None
2025-05-16 14:15:14,867 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675E837740>
2025-05-16 14:15:14,868 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002675D674AD0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:15:14,894 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:15:14,900 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:15:15,006 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675D635880>
2025-05-16 14:15:15,006 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:15:15,006 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:15:15,007 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:15:15,007 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:15:15,008 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:15:15,058 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675D65E720>
2025-05-16 14:15:15,058 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:15:15,059 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:15:15,059 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:15:15,059 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:15:15,059 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:15:15,102 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:15:15 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:15:15,102 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:15:15,102 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:15:15,103 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:15:15,103 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:15:15,103 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:15:15,103 - httpcore.connection - DEBUG - close.started
2025-05-16 14:15:15,103 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:15:15,152 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:15:15 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:15:15,153 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:15:15,153 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:15:15,153 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:15:15,154 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:15:15,154 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:15:15,155 - httpcore.connection - DEBUG - close.started
2025-05-16 14:15:15,155 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:15:16,888 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675D6364B0>
2025-05-16 14:15:16,889 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:15:16,890 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:15:16,891 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:15:16,891 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:15:16,892 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:15:16,892 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:15:16 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-16 14:15:16,892 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:15:16,893 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:15:16,893 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:15:16,893 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:15:16,893 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:15:16,894 - httpcore.connection - DEBUG - close.started
2025-05-16 14:15:16,894 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:15:16,905 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=3 socket_options=None
2025-05-16 14:15:18,973 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675E98B530>
2025-05-16 14:15:18,973 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-05-16 14:15:18,974 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:15:18,975 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-05-16 14:15:18,975 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:15:18,975 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-05-16 14:15:19,155 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:15:18 GMT'), (b'server', b'uvicorn'), (b'content-length', b'16912'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-16 14:15:19,156 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:15:19,156 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-05-16 14:15:19,156 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:15:19,156 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:15:19,156 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:15:19,156 - httpcore.connection - DEBUG - close.started
2025-05-16 14:15:19,157 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:15:19,158 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:15:19,228 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/11" 200 0
2025-05-16 14:15:29,296 - matplotlib - DEBUG - matplotlib data path: C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\matplotlib\mpl-data
2025-05-16 14:15:29,301 - matplotlib - DEBUG - CONFIGDIR=C:\Users\lovel\.matplotlib
2025-05-16 14:15:29,303 - matplotlib - DEBUG - interactive is False
2025-05-16 14:15:29,304 - matplotlib - DEBUG - platform is win32
2025-05-16 14:15:29,361 - matplotlib - DEBUG - CACHEDIR=C:\Users\lovel\.matplotlib
2025-05-16 14:15:29,364 - matplotlib.font_manager - DEBUG - Using fontManager instance from C:\Users\lovel\.matplotlib\fontlist-v390.json
2025-05-16 14:15:29,680 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:29,680 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:29,682 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:29,690 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:29,692 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:29,726 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:29,727 - __main__ - DEBUG - Processing message: hello
2025-05-16 14:15:29,727 - __main__ - DEBUG - Getting response from local model for input: hello
2025-05-16 14:15:29,728 - __main__ - ERROR - Error in get_response: name 'local_chat_response' is not defined
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 216, in get_response
    return local_chat_response(user_input, history)
           ^^^^^^^^^^^^^^^^^^^
NameError: name 'local_chat_response' is not defined. Did you mean: 'openai_chat_response'?
2025-05-16 14:15:29,731 - __main__ - DEBUG - Generated response: An error occurred while generating a response: name 'local_chat_response' is not defined...
2025-05-16 14:15:29,731 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:34,023 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:34,024 - __main__ - INFO - Switched to openai backend
2025-05-16 14:15:34,024 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:36,218 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:36,219 - __main__ - WARNING - Invalid backend: gemini. Keeping current backend: openai
2025-05-16 14:15:36,221 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:38,998 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:38,999 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:39,008 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:39,010 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:39,085 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:39,087 - __main__ - DEBUG - Processing message: hello
2025-05-16 14:15:39,087 - __main__ - DEBUG - Getting response from OpenAI for input: hello
2025-05-16 14:15:39,093 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': "An error occurred while generating a response: name 'local_chat_response' is not defined"}, {'role': 'user', 'content': 'hello'}], 'model': 'gpt-3.5-turbo', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:15:39,135 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:15:39,136 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:15:39,196 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675F07DA30>
2025-05-16 14:15:39,196 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002675C473F50> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:15:39,231 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675F2F9580>
2025-05-16 14:15:39,232 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:15:39,232 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:15:39,232 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:15:39,232 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:15:39,232 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:15:40,541 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:15:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'269'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'277'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199962'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'11ms'), (b'x-request-id', b'req_21060ff69bbdf2a66c6e907c8c3c6c74'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=ozfpvLblSy.MeVCf.gQqws_i_cr019hcIjhoAlN8BaU-1747419340-1.0.1.1-HEM3SCELLYBBLC6yfJElafH0B57TKzVHE4wQjyooC8XmzBIcua0saKCzbpKZxbc3LmHj_HGn89ONXn2xE4539uWxuWhZp_aDjymKAMkQcNo; path=/; expires=Fri, 16-May-25 18:45:40 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=vm07YTR8CDGatXJmDCL9LXCjA8cTwvd8F.pqYc1FXCw-1747419340664-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940cdc972b822340-ORD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:15:40,542 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 14:15:40,542 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:15:40,542 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:15:40,542 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:15:40,542 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:15:40,542 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Fri, 16 May 2025 18:15:40 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-xilu0sqkeveasxfwmh1ozfxa'), ('openai-processing-ms', '269'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '277'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '199962'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '11ms'), ('x-request-id', 'req_21060ff69bbdf2a66c6e907c8c3c6c74'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=ozfpvLblSy.MeVCf.gQqws_i_cr019hcIjhoAlN8BaU-1747419340-1.0.1.1-HEM3SCELLYBBLC6yfJElafH0B57TKzVHE4wQjyooC8XmzBIcua0saKCzbpKZxbc3LmHj_HGn89ONXn2xE4539uWxuWhZp_aDjymKAMkQcNo; path=/; expires=Fri, 16-May-25 18:45:40 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=vm07YTR8CDGatXJmDCL9LXCjA8cTwvd8F.pqYc1FXCw-1747419340664-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '940cdc972b822340-ORD'), ('content-encoding', 'br'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-05-16 14:15:40,542 - openai._base_client - DEBUG - request_id: req_21060ff69bbdf2a66c6e907c8c3c6c74
2025-05-16 14:15:40,544 - __main__ - DEBUG - Generated response: Hello! How can I assist you today?...
2025-05-16 14:15:40,546 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:44,967 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:44,968 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:44,980 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:44,983 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:15:45,025 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:15:45,027 - __main__ - DEBUG - Processing message: which model are you?
2025-05-16 14:15:45,028 - __main__ - DEBUG - Getting response from OpenAI for input: which model are you?
2025-05-16 14:15:45,036 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': "An error occurred while generating a response: name 'local_chat_response' is not defined"}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'which model are you?'}], 'model': 'gpt-3.5-turbo', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:15:45,038 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:15:45,038 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:15:45,039 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:15:45,039 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:15:45,040 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:15:45,040 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:15:45,632 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:15:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'475'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'480'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'199947'), (b'x-ratelimit-reset-requests', b'12.392s'), (b'x-ratelimit-reset-tokens', b'15ms'), (b'x-request-id', b'req_e0cc1d2a56ae39706d7b99b8c5ead63d'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940cdcbb78122340-ORD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:15:45,632 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 14:15:45,632 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:15:45,636 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:15:45,637 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:15:45,637 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:15:45,637 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 16 May 2025 18:15:45 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-xilu0sqkeveasxfwmh1ozfxa', 'openai-processing-ms': '475', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '480', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '199947', 'x-ratelimit-reset-requests': '12.392s', 'x-ratelimit-reset-tokens': '15ms', 'x-request-id': 'req_e0cc1d2a56ae39706d7b99b8c5ead63d', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '940cdcbb78122340-ORD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
2025-05-16 14:15:45,637 - openai._base_client - DEBUG - request_id: req_e0cc1d2a56ae39706d7b99b8c5ead63d
2025-05-16 14:15:45,638 - __main__ - DEBUG - Generated response: I am a text-based AI assistant designed to help answer your questions and provide information to the best of my abilities. How can I assist you today?...
2025-05-16 14:15:45,639 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:03,574 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:03,575 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:03,580 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:03,582 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:03,594 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:03,596 - __main__ - DEBUG - Processing message: which company are you from?
2025-05-16 14:16:03,596 - __main__ - DEBUG - Getting response from OpenAI for input: which company are you from?
2025-05-16 14:16:03,602 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': "An error occurred while generating a response: name 'local_chat_response' is not defined"}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'which model are you?'}, {'role': 'assistant', 'content': 'I am a text-based AI assistant designed to help answer your questions and provide information to the best of my abilities. How can I assist you today?'}, {'role': 'user', 'content': 'which company are you from?'}], 'model': 'gpt-3.5-turbo', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:16:03,603 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:16:03,604 - httpcore.connection - DEBUG - close.started
2025-05-16 14:16:03,604 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:16:03,605 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:16:03,675 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675F3E37A0>
2025-05-16 14:16:03,676 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002675C473F50> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:16:03,698 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675F34E9F0>
2025-05-16 14:16:03,698 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:16:03,698 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:16:03,699 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:16:03,699 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:16:03,699 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:16:04,642 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:16:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'846'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'855'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199901'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'29ms'), (b'x-request-id', b'req_9a45359fbf8cc4aa754714bd7d91980a'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940cdd3009f74295-EWR'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:16:04,643 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 14:16:04,643 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:16:04,648 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:16:04,648 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:16:04,648 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:16:04,649 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 16 May 2025 18:16:04 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-xilu0sqkeveasxfwmh1ozfxa', 'openai-processing-ms': '846', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '855', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199901', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '29ms', 'x-request-id': 'req_9a45359fbf8cc4aa754714bd7d91980a', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '940cdd3009f74295-EWR', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
2025-05-16 14:16:04,649 - openai._base_client - DEBUG - request_id: req_9a45359fbf8cc4aa754714bd7d91980a
2025-05-16 14:16:04,649 - __main__ - DEBUG - Generated response: I am a virtual assistant created by an AI development team. I am here to assist you with any questions or information you may need. How can I help you today?...
2025-05-16 14:16:04,650 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:12,367 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:12,368 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:12,379 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:12,380 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:12,411 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:12,412 - __main__ - DEBUG - Processing message: which AI team, what is the name?
2025-05-16 14:16:12,412 - __main__ - DEBUG - Getting response from OpenAI for input: which AI team, what is the name?
2025-05-16 14:16:12,419 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': "An error occurred while generating a response: name 'local_chat_response' is not defined"}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'which model are you?'}, {'role': 'assistant', 'content': 'I am a text-based AI assistant designed to help answer your questions and provide information to the best of my abilities. How can I assist you today?'}, {'role': 'user', 'content': 'which company are you from?'}, {'role': 'assistant', 'content': 'I am a virtual assistant created by an AI development team. I am here to assist you with any questions or information you may need. How can I help you today?'}, {'role': 'user', 'content': 'which AI team, what is the name?'}], 'model': 'gpt-3.5-turbo', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:16:12,419 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:16:12,419 - httpcore.connection - DEBUG - close.started
2025-05-16 14:16:12,419 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:16:12,419 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:16:12,483 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675F3F13D0>
2025-05-16 14:16:12,484 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002675C473F50> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:16:12,521 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675F421BE0>
2025-05-16 14:16:12,522 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:16:12,522 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:16:12,522 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:16:12,522 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:16:12,522 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:16:14,549 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:16:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'1284'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1909'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199852'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'44ms'), (b'x-request-id', b'req_1b250d24d990018546740bd08074bf89'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940cdd673d29ff5b-ORD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:16:14,549 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 14:16:14,550 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:16:14,551 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:16:14,552 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:16:14,552 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:16:14,552 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 16 May 2025 18:16:14 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-xilu0sqkeveasxfwmh1ozfxa', 'openai-processing-ms': '1284', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1909', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199852', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '44ms', 'x-request-id': 'req_1b250d24d990018546740bd08074bf89', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '940cdd673d29ff5b-ORD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
2025-05-16 14:16:14,552 - openai._base_client - DEBUG - request_id: req_1b250d24d990018546740bd08074bf89
2025-05-16 14:16:14,552 - __main__ - DEBUG - Generated response: I am a generic AI assistant created by a team of developers. I do not have a specific company or team name associated with me. How can I assist you today?...
2025-05-16 14:16:14,553 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:37,815 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:37,816 - __main__ - INFO - Switched to local backend
2025-05-16 14:16:37,816 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:45,152 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:45,153 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:45,167 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:45,168 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:45,187 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:45,188 - __main__ - DEBUG - Processing message: and what can you do?
2025-05-16 14:16:45,188 - __main__ - DEBUG - Getting response from local model for input: and what can you do?
2025-05-16 14:16:45,188 - __main__ - ERROR - Error in get_response: name 'local_chat_response' is not defined
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 216, in get_response
    return local_chat_response(user_input, history)
           ^^^^^^^^^^^^^^^^^^^
NameError: name 'local_chat_response' is not defined. Did you mean: 'openai_chat_response'?
2025-05-16 14:16:45,191 - __main__ - DEBUG - Generated response: An error occurred while generating a response: name 'local_chat_response' is not defined...
2025-05-16 14:16:45,192 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:50,075 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:50,076 - __main__ - INFO - Switched to openai backend
2025-05-16 14:16:50,076 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:53,370 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:53,372 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:53,383 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:53,384 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:16:53,436 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:16:53,437 - __main__ - DEBUG - Processing message: and you
2025-05-16 14:16:53,437 - __main__ - DEBUG - Getting response from OpenAI for input: and you
2025-05-16 14:16:53,446 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': "An error occurred while generating a response: name 'local_chat_response' is not defined"}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}, {'role': 'user', 'content': 'which model are you?'}, {'role': 'assistant', 'content': 'I am a text-based AI assistant designed to help answer your questions and provide information to the best of my abilities. How can I assist you today?'}, {'role': 'user', 'content': 'which company are you from?'}, {'role': 'assistant', 'content': 'I am a virtual assistant created by an AI development team. I am here to assist you with any questions or information you may need. How can I help you today?'}, {'role': 'user', 'content': 'which AI team, what is the name?'}, {'role': 'assistant', 'content': 'I am a generic AI assistant created by a team of developers. I do not have a specific company or team name associated with me. How can I assist you today?'}, {'role': 'user', 'content': 'and what can you do?'}, {'role': 'assistant', 'content': "An error occurred while generating a response: name 'local_chat_response' is not defined"}, {'role': 'user', 'content': 'and you'}], 'model': 'gpt-3.5-turbo', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:16:53,447 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:16:53,447 - httpcore.connection - DEBUG - close.started
2025-05-16 14:16:53,448 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:16:53,448 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:16:53,484 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675F42B7D0>
2025-05-16 14:16:53,485 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002675C473F50> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:16:53,519 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002675F419820>
2025-05-16 14:16:53,519 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:16:53,519 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:16:53,519 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:16:53,519 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:16:53,519 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:16:55,541 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:16:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'1663'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1922'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199781'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'65ms'), (b'x-request-id', b'req_78551d584c53a44fde2b9c0fed8629c7'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940cde677b11e11c-ORD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:16:55,542 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 14:16:55,542 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:16:55,550 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:16:55,551 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:16:55,551 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:16:55,551 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 16 May 2025 18:16:55 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-xilu0sqkeveasxfwmh1ozfxa', 'openai-processing-ms': '1663', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1922', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '199781', 'x-ratelimit-reset-requests': '8.64s', 'x-ratelimit-reset-tokens': '65ms', 'x-request-id': 'req_78551d584c53a44fde2b9c0fed8629c7', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '940cde677b11e11c-ORD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
2025-05-16 14:16:55,551 - openai._base_client - DEBUG - request_id: req_78551d584c53a44fde2b9c0fed8629c7
2025-05-16 14:16:55,551 - __main__ - DEBUG - Generated response: I can help answer questions, provide information, offer suggestions, and assist with various tasks within my capabilities. Feel free to ask me anything you'd like help with!...
2025-05-16 14:16:55,552 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:17:02,468 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:17:02,469 - __main__ - INFO - Switched to local backend
2025-05-16 14:17:02,469 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:17:03,656 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:17:03,657 - __main__ - WARNING - Invalid backend: gemini. Keeping current backend: local
2025-05-16 14:17:03,657 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:17:06,819 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:17:06,821 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:17:06,831 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:17:06,833 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:17:06,873 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:17:06,875 - __main__ - DEBUG - Processing message: you there
2025-05-16 14:17:06,875 - __main__ - DEBUG - Getting response from local model for input: you there
2025-05-16 14:17:06,875 - __main__ - ERROR - Error in get_response: name 'local_chat_response' is not defined
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 216, in get_response
    return local_chat_response(user_input, history)
           ^^^^^^^^^^^^^^^^^^^
NameError: name 'local_chat_response' is not defined. Did you mean: 'openai_chat_response'?
2025-05-16 14:17:06,879 - __main__ - DEBUG - Generated response: An error occurred while generating a response: name 'local_chat_response' is not defined...
2025-05-16 14:17:06,880 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:06,543 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:19:06,544 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:19:06,544 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:19:06,544 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:19:06,546 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:19:10,637 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002047997ECC0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:19:10,638 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:19:10,641 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:19:10,812 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:19:10,962 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:19:10,970 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:19:11,000 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:19:11,007 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:19:11,025 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002047ADCD5E0>
2025-05-16 14:19:11,025 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000204799772D0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:19:11,025 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002047AD31190>
2025-05-16 14:19:11,026 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020479AC8750> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:19:11,050 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002047ADCD4F0>
2025-05-16 14:19:11,050 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:19:11,051 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002047ADCDBB0>
2025-05-16 14:19:11,051 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:19:11,051 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:19:11,051 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:19:11,051 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:19:11,052 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:19:11,052 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:19:11,052 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:19:11,053 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:19:11,053 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:19:11,072 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:19:11 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:19:11,072 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:19:11,072 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:19:11 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:19:11,072 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:19:11,072 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:19:11,074 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:19:11,074 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:19:11,074 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:19:11,074 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:19:11,074 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:19:11,074 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:19:11,075 - httpcore.connection - DEBUG - close.started
2025-05-16 14:19:11,075 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:19:11,075 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:19:11,075 - httpcore.connection - DEBUG - close.started
2025-05-16 14:19:11,076 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:19:11,076 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:19:11,077 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:19:11,078 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=None socket_options=None
2025-05-16 14:19:11,087 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002047ADCCF20>
2025-05-16 14:19:11,087 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020479977650> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:19:11,099 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002047ADCE3C0>
2025-05-16 14:19:11,100 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020479AC89D0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:19:11,158 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:19:11,208 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:19:11,280 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000204792E3560>
2025-05-16 14:19:11,281 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:19:11,281 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:19:11,281 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:19:11,282 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:19:11,282 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:19:11,293 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002047997EFF0>
2025-05-16 14:19:11,293 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:19:11,293 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:19:11,294 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:19:11,294 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:19:11,294 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:19:11,376 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:19:11 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:19:11,376 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:19:11,377 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:19:11,377 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:19:11,377 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:19:11,377 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:19:11,377 - httpcore.connection - DEBUG - close.started
2025-05-16 14:19:11,378 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:19:11,388 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:19:11 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:19:11,388 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:19:11,389 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:19:11,389 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:19:11,390 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:19:11,390 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:19:11,390 - httpcore.connection - DEBUG - close.started
2025-05-16 14:19:11,390 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:19:13,114 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020479AB0050>
2025-05-16 14:19:13,115 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:19:13,115 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:19:13,116 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:19:13,116 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:19:13,117 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:19:13,117 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:19:13 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-16 14:19:13,117 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:19:13,117 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:19:13,117 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:19:13,117 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:19:13,117 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:19:13,117 - httpcore.connection - DEBUG - close.started
2025-05-16 14:19:13,117 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:19:13,125 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=3 socket_options=None
2025-05-16 14:19:15,170 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002047AE00800>
2025-05-16 14:19:15,171 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-05-16 14:19:15,173 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:19:15,174 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-05-16 14:19:15,174 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:19:15,174 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-05-16 14:19:15,185 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:19:15 GMT'), (b'server', b'uvicorn'), (b'content-length', b'16911'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-16 14:19:15,185 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:19:15,185 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-05-16 14:19:15,186 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:19:15,186 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:19:15,186 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:19:15,186 - httpcore.connection - DEBUG - close.started
2025-05-16 14:19:15,187 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:19:15,189 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:19:15,283 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/11" 200 0
2025-05-16 14:19:33,687 - matplotlib - DEBUG - matplotlib data path: C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\matplotlib\mpl-data
2025-05-16 14:19:33,692 - matplotlib - DEBUG - CONFIGDIR=C:\Users\lovel\.matplotlib
2025-05-16 14:19:33,693 - matplotlib - DEBUG - interactive is False
2025-05-16 14:19:33,693 - matplotlib - DEBUG - platform is win32
2025-05-16 14:19:33,746 - matplotlib - DEBUG - CACHEDIR=C:\Users\lovel\.matplotlib
2025-05-16 14:19:33,748 - matplotlib.font_manager - DEBUG - Using fontManager instance from C:\Users\lovel\.matplotlib\fontlist-v390.json
2025-05-16 14:19:33,931 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:33,932 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:33,932 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:33,943 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:33,944 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:33,996 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:33,996 - __main__ - DEBUG - Processing message: hello
2025-05-16 14:19:33,997 - __main__ - DEBUG - Using backend: local for input: hello
2025-05-16 14:19:33,997 - __main__ - DEBUG - Using local backend
2025-05-16 14:19:33,997 - __main__ - DEBUG - Generated response: Local model received: hello...
2025-05-16 14:19:33,998 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:39,349 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:39,350 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:39,358 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:39,360 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:39,375 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:39,377 - __main__ - DEBUG - Processing message: what is 2 plus 2
2025-05-16 14:19:39,377 - __main__ - DEBUG - Using backend: local for input: what is 2 plus 2
2025-05-16 14:19:39,378 - __main__ - DEBUG - Using local backend
2025-05-16 14:19:39,378 - __main__ - DEBUG - Generated response: Local model received: what is 2 plus 2...
2025-05-16 14:19:39,379 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:45,520 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:45,523 - __main__ - INFO - Switched to openai backend
2025-05-16 14:19:45,523 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:48,956 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:48,957 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:48,968 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:48,970 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:49,031 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:49,033 - __main__ - DEBUG - Processing message: what is 2 plus 2
2025-05-16 14:19:49,033 - __main__ - DEBUG - Using backend: openai for input: what is 2 plus 2
2025-05-16 14:19:49,037 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'Local model received: hello'}, {'role': 'user', 'content': 'what is 2 plus 2'}, {'role': 'assistant', 'content': 'Local model received: what is 2 plus 2'}, {'role': 'user', 'content': 'what is 2 plus 2'}], 'model': 'gpt-3.5-turbo', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:19:49,099 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:19:49,100 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:19:49,186 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002047B6C26F0>
2025-05-16 14:19:49,186 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000204788F7E50> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:19:49,224 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002047B809640>
2025-05-16 14:19:49,225 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:19:49,225 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:19:49,225 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:19:49,225 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:19:49,225 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:19:49,565 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:19:49 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'230'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'232'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'199960'), (b'x-ratelimit-reset-requests', b'8.64s'), (b'x-ratelimit-reset-tokens', b'12ms'), (b'x-request-id', b'req_060fc8010881c29170c55707274079e8'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=WCyax1Ot9SohfTQf_5imBebrsdKOsJlPuTEFOJ9s_Ac-1747419589-1.0.1.1-Z9Jqr.2wiVOvmtbngaHc_FTa8sg5i2QK0yvvm3t7rkMB9E0us5mU6U0mVqCNjooUGgRvtKfeyfW3Llw2sWsjeUh36jQeaXLLoqLzVIja8X8; path=/; expires=Fri, 16-May-25 18:49:49 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=ErMEEErKllR_zv9lMEwtN1POU3NDREum5ragM44auSU-1747419589691-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940ce2b1bdbd1150-ORD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:19:49,566 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 14:19:49,566 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:19:49,599 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:19:49,599 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:19:49,599 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:19:49,600 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Fri, 16 May 2025 18:19:49 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-xilu0sqkeveasxfwmh1ozfxa'), ('openai-processing-ms', '230'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '232'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '199960'), ('x-ratelimit-reset-requests', '8.64s'), ('x-ratelimit-reset-tokens', '12ms'), ('x-request-id', 'req_060fc8010881c29170c55707274079e8'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=WCyax1Ot9SohfTQf_5imBebrsdKOsJlPuTEFOJ9s_Ac-1747419589-1.0.1.1-Z9Jqr.2wiVOvmtbngaHc_FTa8sg5i2QK0yvvm3t7rkMB9E0us5mU6U0mVqCNjooUGgRvtKfeyfW3Llw2sWsjeUh36jQeaXLLoqLzVIja8X8; path=/; expires=Fri, 16-May-25 18:49:49 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=ErMEEErKllR_zv9lMEwtN1POU3NDREum5ragM44auSU-1747419589691-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '940ce2b1bdbd1150-ORD'), ('content-encoding', 'br'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-05-16 14:19:49,600 - openai._base_client - DEBUG - request_id: req_060fc8010881c29170c55707274079e8
2025-05-16 14:19:49,601 - __main__ - DEBUG - Generated response: 2 plus 2 is equal to 4....
2025-05-16 14:19:49,602 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:51,943 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:51,944 - __main__ - WARNING - Invalid backend: gemini. Keeping current backend: openai
2025-05-16 14:19:51,944 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:54,360 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:54,361 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:54,371 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:54,372 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:19:54,445 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:19:54,446 - __main__ - DEBUG - Processing message: and you?
2025-05-16 14:19:54,447 - __main__ - DEBUG - Using backend: openai for input: and you?
2025-05-16 14:19:54,452 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'Local model received: hello'}, {'role': 'user', 'content': 'what is 2 plus 2'}, {'role': 'assistant', 'content': 'Local model received: what is 2 plus 2'}, {'role': 'user', 'content': 'what is 2 plus 2'}, {'role': 'assistant', 'content': '2 plus 2 is equal to 4.'}, {'role': 'user', 'content': 'and you?'}], 'model': 'gpt-3.5-turbo', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:19:54,453 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:19:54,453 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:19:54,454 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:19:54,454 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:19:54,454 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:19:54,455 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:19:55,181 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:19:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'615'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'623'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'9998'), (b'x-ratelimit-remaining-tokens', b'199950'), (b'x-ratelimit-reset-requests', b'12.057s'), (b'x-ratelimit-reset-tokens', b'15ms'), (b'x-request-id', b'req_b49e7551cd9c022a48488967115e93e2'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940ce2d25e9a1150-ORD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:19:55,181 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 14:19:55,182 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:19:55,185 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:19:55,186 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:19:55,186 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:19:55,186 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 16 May 2025 18:19:55 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-xilu0sqkeveasxfwmh1ozfxa', 'openai-processing-ms': '615', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '623', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '9998', 'x-ratelimit-remaining-tokens': '199950', 'x-ratelimit-reset-requests': '12.057s', 'x-ratelimit-reset-tokens': '15ms', 'x-request-id': 'req_b49e7551cd9c022a48488967115e93e2', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '940ce2d25e9a1150-ORD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
2025-05-16 14:19:55,186 - openai._base_client - DEBUG - request_id: req_b49e7551cd9c022a48488967115e93e2
2025-05-16 14:19:55,187 - __main__ - DEBUG - Generated response: I do not have the ability to perform math calculations as I am a text-based assistant. If you have any other questions or need assistance with something else, feel free to ask....
2025-05-16 14:19:55,188 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:24:10,464 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:24:10,464 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:24:10,464 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:24:10,464 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:24:10,465 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:24:14,572 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002293E6F0380>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:24:14,572 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:24:14,575 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:24:14,755 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:24:14,881 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:24:14,883 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:24:14,925 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:24:14,927 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:24:14,946 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002293FB121E0>
2025-05-16 14:24:14,946 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002293E6CB350> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:24:14,951 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002293E9707A0>
2025-05-16 14:24:14,951 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002293E8187D0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:24:14,970 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002293FABEF30>
2025-05-16 14:24:14,970 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:24:14,970 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:24:14,970 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:24:14,970 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:24:14,970 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:24:14,976 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002293FB12ED0>
2025-05-16 14:24:14,976 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:24:14,976 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:24:14,976 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:24:14,977 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:24:14,977 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:24:14,978 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002293FB12900>
2025-05-16 14:24:14,978 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002293E6CB6D0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:24:14,992 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:24:15 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:24:14,993 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:24:14,993 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:24:14,993 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:24:14,993 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:24:14,993 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:24:14,993 - httpcore.connection - DEBUG - close.started
2025-05-16 14:24:14,994 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:24:14,995 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:24:14,999 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:24:15 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:24:15,000 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:24:15,000 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:24:15,000 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:24:15,000 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:24:15,000 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:24:15,000 - httpcore.connection - DEBUG - close.started
2025-05-16 14:24:15,002 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:24:15,002 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:24:15,019 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=None socket_options=None
2025-05-16 14:24:15,039 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002293FB13680>
2025-05-16 14:24:15,039 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002293E818A50> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:24:15,076 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:24:15,119 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:24:15,177 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002293DFD5160>
2025-05-16 14:24:15,177 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:24:15,178 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:24:15,178 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:24:15,178 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:24:15,179 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:24:15,234 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002293E824380>
2025-05-16 14:24:15,234 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:24:15,235 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:24:15,235 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:24:15,235 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:24:15,235 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:24:15,272 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:24:15 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:24:15,272 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:24:15,273 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:24:15,273 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:24:15,273 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:24:15,273 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:24:15,273 - httpcore.connection - DEBUG - close.started
2025-05-16 14:24:15,274 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:24:15,334 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:24:15 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:24:15,334 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:24:15,334 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:24:15,335 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:24:15,335 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:24:15,335 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:24:15,335 - httpcore.connection - DEBUG - close.started
2025-05-16 14:24:15,336 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:24:17,060 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002293E7FA5D0>
2025-05-16 14:24:17,061 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:24:17,063 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:24:17,065 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:24:17,065 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:24:17,065 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:24:17,066 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:24:16 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-16 14:24:17,067 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:24:17,067 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:24:17,067 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:24:17,067 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:24:17,067 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:24:17,067 - httpcore.connection - DEBUG - close.started
2025-05-16 14:24:17,069 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:24:17,075 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=3 socket_options=None
2025-05-16 14:24:19,142 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002293FB4CEF0>
2025-05-16 14:24:19,143 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-05-16 14:24:19,143 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:24:19,144 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-05-16 14:24:19,145 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:24:19,145 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-05-16 14:24:19,158 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:24:19 GMT'), (b'server', b'uvicorn'), (b'content-length', b'16911'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-16 14:24:19,158 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:24:19,158 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-05-16 14:24:19,158 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:24:19,158 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:24:19,158 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:24:19,158 - httpcore.connection - DEBUG - close.started
2025-05-16 14:24:19,160 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:24:19,161 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:24:19,230 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/11" 200 0
2025-05-16 14:27:10,912 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:27:10,912 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:27:10,912 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:27:10,913 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:27:10,913 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:27:15,005 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001E0E59F3F80>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:27:15,005 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:27:15,008 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:27:15,175 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:27:15,306 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:27:15,320 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:27:15,349 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:27:15,355 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:27:15,374 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E6E4AD50>
2025-05-16 14:27:15,374 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E0E5B48750> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:27:15,383 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E6E4A6C0>
2025-05-16 14:27:15,383 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E0E59FB750> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:27:15,394 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E6E4AC60>
2025-05-16 14:27:15,394 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:27:15,396 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:27:15,396 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:27:15,396 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:27:15,396 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:27:15,406 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E6E4A5D0>
2025-05-16 14:27:15,407 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:27:15,407 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:27:15,407 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:27:15,408 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:27:15,408 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:27:15,415 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E6E4A000>
2025-05-16 14:27:15,415 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E0E59FB5D0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:27:15,423 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:27:15 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:27:15,424 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:27:15,424 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:27:15,424 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:27:15,424 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:27:15,424 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:27:15,425 - httpcore.connection - DEBUG - close.started
2025-05-16 14:27:15,425 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:27:15,426 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:27:15,428 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:27:15 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:27:15,428 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:27:15,428 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:27:15,428 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:27:15,429 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:27:15,429 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:27:15,429 - httpcore.connection - DEBUG - close.started
2025-05-16 14:27:15,429 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:27:15,430 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:27:15,437 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=None socket_options=None
2025-05-16 14:27:15,448 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E6E4B4A0>
2025-05-16 14:27:15,448 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E0E5B489D0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:27:15,519 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:27:15,521 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:27:15,607 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E5C99DF0>
2025-05-16 14:27:15,607 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:27:15,608 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:27:15,608 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:27:15,608 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:27:15,608 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:27:15,647 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E5C6C0B0>
2025-05-16 14:27:15,648 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:27:15,648 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:27:15,648 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:27:15,649 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:27:15,649 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:27:15,705 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:27:15 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:27:15,706 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:27:15,706 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:27:15,706 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:27:15,707 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:27:15,707 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:27:15,707 - httpcore.connection - DEBUG - close.started
2025-05-16 14:27:15,707 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:27:15,744 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:27:15 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:27:15,744 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:27:15,744 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:27:15,744 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:27:15,746 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:27:15,746 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:27:15,746 - httpcore.connection - DEBUG - close.started
2025-05-16 14:27:15,746 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:27:17,470 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E5A2D040>
2025-05-16 14:27:17,470 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:27:17,470 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:27:17,472 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:27:17,473 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:27:17,473 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:27:17,473 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:27:17 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-16 14:27:17,473 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:27:17,473 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:27:17,473 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:27:17,474 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:27:17,474 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:27:17,474 - httpcore.connection - DEBUG - close.started
2025-05-16 14:27:17,474 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:27:17,481 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=3 socket_options=None
2025-05-16 14:27:19,556 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E6E70BC0>
2025-05-16 14:27:19,557 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-05-16 14:27:19,557 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:27:19,558 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-05-16 14:27:19,558 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:27:19,558 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-05-16 14:27:19,568 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:27:19 GMT'), (b'server', b'uvicorn'), (b'content-length', b'16876'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-16 14:27:19,569 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:27:19,569 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-05-16 14:27:19,569 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:27:19,569 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:27:19,569 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:27:19,569 - httpcore.connection - DEBUG - close.started
2025-05-16 14:27:19,570 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:27:19,572 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:27:19,635 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/11" 200 0
2025-05-16 14:27:29,850 - matplotlib - DEBUG - matplotlib data path: C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\matplotlib\mpl-data
2025-05-16 14:27:29,855 - matplotlib - DEBUG - CONFIGDIR=C:\Users\lovel\.matplotlib
2025-05-16 14:27:29,856 - matplotlib - DEBUG - interactive is False
2025-05-16 14:27:29,856 - matplotlib - DEBUG - platform is win32
2025-05-16 14:27:29,908 - matplotlib - DEBUG - CACHEDIR=C:\Users\lovel\.matplotlib
2025-05-16 14:27:29,910 - matplotlib.font_manager - DEBUG - Using fontManager instance from C:\Users\lovel\.matplotlib\fontlist-v390.json
2025-05-16 14:27:30,101 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:27:30,101 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:27:30,102 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:27:30,113 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:27:30,114 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:27:30,157 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:27:30,158 - __main__ - DEBUG - Processing message: can you do it
2025-05-16 14:27:30,158 - __main__ - DEBUG - Using backend: gemini for input: can you do it
2025-05-16 14:27:31,145 - __main__ - DEBUG - Generated response: Yes....
2025-05-16 14:27:31,146 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:27:37,746 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:27:37,748 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:27:37,759 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:27:37,760 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:27:37,836 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:27:37,839 - __main__ - DEBUG - Processing message: what model are you and what are you good at?
2025-05-16 14:27:37,839 - __main__ - DEBUG - Using backend: gemini for input: what model are you and what are you good at?
2025-05-16 14:27:43,722 - __main__ - DEBUG - Generated response: I am a large language model, trained by Google.

I'm good at a wide range of tasks involving language and information, including:

*   **Answering questions:** Accessing and processing information fro...
2025-05-16 14:27:43,723 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:27:56,919 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:27:56,921 - __main__ - INFO - Switched to openai backend
2025-05-16 14:27:56,922 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:28:01,364 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:28:01,365 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:28:01,373 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:28:01,374 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:28:01,402 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:28:01,404 - __main__ - DEBUG - Processing message: same question to you?
2025-05-16 14:28:01,404 - __main__ - DEBUG - Using backend: openai for input: same question to you?
2025-05-16 14:28:01,408 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'can you do it'}, {'role': 'assistant', 'content': 'Yes.'}, {'role': 'user', 'content': 'what model are you and what are you good at?'}, {'role': 'assistant', 'content': "I am a large language model, trained by Google.\n\nI'm good at a wide range of tasks involving language and information, including:\n\n*   **Answering questions:** Accessing and processing information from the real world to provide comprehensive and informative answers.\n*   **Generating text:** Writing stories, poems, code, scripts, musical pieces, email, letters, etc.\n*   **Summarizing information:** Condensing long texts into shorter summaries.\n*   **Translating languages:** Converting text from one language to another.\n*   **Explaining concepts:** Breaking down complex topics into understandable terms.\n*   **Helping with creative tasks:** Brainstorming ideas, writing drafts, and assisting with various forms of content creation.\n*   **Following instructions:** Performing tasks based on the prompts you give me.\n*   **Engaging in conversation:** Chatting on various topics.\n\nBasically, anything that involves understanding, processing, or generating text is something I can likely help with!"}, {'role': 'user', 'content': 'same question to you?'}], 'model': 'o4-mini', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:28:01,475 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:28:01,476 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:28:01,534 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E6DEF080>
2025-05-16 14:28:01,535 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E0E494FE50> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:28:01,563 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E768D6A0>
2025-05-16 14:28:01,563 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:28:01,565 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:28:01,565 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:28:01,565 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:28:01,565 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:28:01,672 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Fri, 16 May 2025 18:28:01 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'245'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'20'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'31'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'199715'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'85ms'), (b'x-request-id', b'req_f496b0572b8a83b5436926a0c3bf92e0'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=0.6AjwCPizG5kyt_DTG_3WPnP9_DRb3IJsk0Tf3E.ko-1747420081-1.0.1.1-g7b_VEh2bUWXA58rGZ3UmpFtD8KJ5GFRaP2OFbrCvRpzmDwyxUs5C4YMwiNe8oJORiTleiI7fxaPs0bdVPlno1RCmy8nztPdbajuHl4z11w; path=/; expires=Fri, 16-May-25 18:58:01 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=jHZW3_mU_rvN6YyjiCdGUOgWon6Tx4.B4QjmcgD3Adk-1747420081810-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940ceeb6cc94659d-EWR'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:28:01,673 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-16 14:28:01,673 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:28:01,673 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:28:01,673 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:28:01,673 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:28:01,673 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "400 Bad Request" Headers([('date', 'Fri, 16 May 2025 18:28:01 GMT'), ('content-type', 'application/json'), ('content-length', '245'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-xilu0sqkeveasxfwmh1ozfxa'), ('openai-processing-ms', '20'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '31'), ('x-ratelimit-limit-requests', '500'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '499'), ('x-ratelimit-remaining-tokens', '199715'), ('x-ratelimit-reset-requests', '120ms'), ('x-ratelimit-reset-tokens', '85ms'), ('x-request-id', 'req_f496b0572b8a83b5436926a0c3bf92e0'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=0.6AjwCPizG5kyt_DTG_3WPnP9_DRb3IJsk0Tf3E.ko-1747420081-1.0.1.1-g7b_VEh2bUWXA58rGZ3UmpFtD8KJ5GFRaP2OFbrCvRpzmDwyxUs5C4YMwiNe8oJORiTleiI7fxaPs0bdVPlno1RCmy8nztPdbajuHl4z11w; path=/; expires=Fri, 16-May-25 18:58:01 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=jHZW3_mU_rvN6YyjiCdGUOgWon6Tx4.B4QjmcgD3Adk-1747420081810-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '940ceeb6cc94659d-EWR'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-05-16 14:28:01,674 - openai._base_client - DEBUG - request_id: req_f496b0572b8a83b5436926a0c3bf92e0
2025-05-16 14:28:01,674 - openai._base_client - DEBUG - Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1043, in _request
    response.raise_for_status()
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.openai.com/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400
2025-05-16 14:28:01,675 - openai._base_client - DEBUG - Not retrying
2025-05-16 14:28:01,675 - openai._base_client - DEBUG - Re-raising status error
2025-05-16 14:28:01,676 - __main__ - ERROR - OpenAI API Error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 130, in openai_chat_response
    response = openai_client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_utils\_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\resources\chat\completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-05-16 14:28:01,677 - __main__ - DEBUG - Generated response: I'm sorry, I encountered an error processing your request....
2025-05-16 14:28:01,677 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:28:12,157 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:28:12,158 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:28:12,172 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:28:12,173 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:28:12,218 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:28:12,220 - __main__ - DEBUG - Processing message: what model are you and which company?
2025-05-16 14:28:12,220 - __main__ - DEBUG - Using backend: openai for input: what model are you and which company?
2025-05-16 14:28:12,231 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant.'}, {'role': 'user', 'content': 'can you do it'}, {'role': 'assistant', 'content': 'Yes.'}, {'role': 'user', 'content': 'what model are you and what are you good at?'}, {'role': 'assistant', 'content': "I am a large language model, trained by Google.\n\nI'm good at a wide range of tasks involving language and information, including:\n\n*   **Answering questions:** Accessing and processing information from the real world to provide comprehensive and informative answers.\n*   **Generating text:** Writing stories, poems, code, scripts, musical pieces, email, letters, etc.\n*   **Summarizing information:** Condensing long texts into shorter summaries.\n*   **Translating languages:** Converting text from one language to another.\n*   **Explaining concepts:** Breaking down complex topics into understandable terms.\n*   **Helping with creative tasks:** Brainstorming ideas, writing drafts, and assisting with various forms of content creation.\n*   **Following instructions:** Performing tasks based on the prompts you give me.\n*   **Engaging in conversation:** Chatting on various topics.\n\nBasically, anything that involves understanding, processing, or generating text is something I can likely help with!"}, {'role': 'user', 'content': 'same question to you?'}, {'role': 'assistant', 'content': "I'm sorry, I encountered an error processing your request."}, {'role': 'user', 'content': 'what model are you and which company?'}], 'model': 'o4-mini', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:28:12,232 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:28:12,232 - httpcore.connection - DEBUG - close.started
2025-05-16 14:28:12,232 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:28:12,232 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:28:12,253 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E7C80B90>
2025-05-16 14:28:12,253 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001E0E494FE50> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:28:12,273 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001E0E7C804D0>
2025-05-16 14:28:12,273 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:28:12,274 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:28:12,274 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:28:12,274 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:28:12,274 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:28:12,449 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Fri, 16 May 2025 18:28:12 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'245'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'29'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'38'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'199690'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'93ms'), (b'x-request-id', b'req_a1cf4ffedc35fb9578857e6ab53462ab'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940ceef9a8fc42f4-EWR'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:28:12,450 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-16 14:28:12,450 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:28:12,450 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:28:12,451 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:28:12,451 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:28:12,451 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "400 Bad Request" Headers({'date': 'Fri, 16 May 2025 18:28:12 GMT', 'content-type': 'application/json', 'content-length': '245', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-xilu0sqkeveasxfwmh1ozfxa', 'openai-processing-ms': '29', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '38', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '199690', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '93ms', 'x-request-id': 'req_a1cf4ffedc35fb9578857e6ab53462ab', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '940ceef9a8fc42f4-EWR', 'alt-svc': 'h3=":443"; ma=86400'})
2025-05-16 14:28:12,451 - openai._base_client - DEBUG - request_id: req_a1cf4ffedc35fb9578857e6ab53462ab
2025-05-16 14:28:12,451 - openai._base_client - DEBUG - Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1043, in _request
    response.raise_for_status()
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.openai.com/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400
2025-05-16 14:28:12,452 - openai._base_client - DEBUG - Not retrying
2025-05-16 14:28:12,452 - openai._base_client - DEBUG - Re-raising status error
2025-05-16 14:28:12,453 - __main__ - ERROR - OpenAI API Error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 130, in openai_chat_response
    response = openai_client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_utils\_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\resources\chat\completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-05-16 14:28:12,453 - __main__ - DEBUG - Generated response: I'm sorry, I encountered an error processing your request....
2025-05-16 14:28:12,454 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:39:55,979 - __main__ - DEBUG - OpenAI client initialized successfully
2025-05-16 14:39:56,733 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:39:56,733 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:39:56,733 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:39:56,734 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:39:56,735 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:40:00,842 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000022F106D8F20>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:40:00,843 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:40:00,848 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:40:01,015 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:40:01,177 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:40:01,179 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:40:01,222 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:40:01,224 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:40:01,233 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F12282A50>
2025-05-16 14:40:01,233 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022F10E3B350> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:40:01,244 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F122837A0>
2025-05-16 14:40:01,246 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022F10F888D0> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:40:01,266 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F12282960>
2025-05-16 14:40:01,266 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:40:01,267 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:40:01,267 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:40:01,268 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:40:01,268 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:40:01,286 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F122836B0>
2025-05-16 14:40:01,286 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:40:01,287 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:40:01,287 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:40:01,287 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:40:01,287 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:40:01,299 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=None socket_options=None
2025-05-16 14:40:01,303 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:40:01 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:40:01,303 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:40:01,303 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:40:01,304 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:40:01,304 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:40:01,304 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:40:01,304 - httpcore.connection - DEBUG - close.started
2025-05-16 14:40:01,304 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:40:01,305 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:40:01,307 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:40:01 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:40:01,307 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:40:01,307 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:40:01,307 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:40:01,307 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:40:01,307 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:40:01,308 - httpcore.connection - DEBUG - close.started
2025-05-16 14:40:01,308 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:40:01,308 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:40:01,311 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F12281BE0>
2025-05-16 14:40:01,311 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022F10E3B6D0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:40:01,342 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F12283110>
2025-05-16 14:40:01,342 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022F10F88850> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:40:01,391 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:40:01,392 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:40:01,499 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F10ECD790>
2025-05-16 14:40:01,499 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:40:01,500 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:40:01,500 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:40:01,500 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:40:01,500 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:40:01,535 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F10F8C6E0>
2025-05-16 14:40:01,535 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:40:01,536 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:40:01,536 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:40:01,536 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:40:01,536 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:40:01,595 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:40:01 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:40:01,596 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:40:01,596 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:40:01,596 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:40:01,596 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:40:01,596 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:40:01,596 - httpcore.connection - DEBUG - close.started
2025-05-16 14:40:01,596 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:40:01,629 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:40:01 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:40:01,630 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:40:01,630 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:40:01,630 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:40:01,630 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:40:01,631 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:40:01,631 - httpcore.connection - DEBUG - close.started
2025-05-16 14:40:01,631 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:40:03,346 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F10F8C350>
2025-05-16 14:40:03,346 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:40:03,347 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:40:03,347 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:40:03,348 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:40:03,348 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:40:03,348 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:40:03 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-16 14:40:03,348 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:40:03,349 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:40:03,349 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:40:03,349 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:40:03,349 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:40:03,349 - httpcore.connection - DEBUG - close.started
2025-05-16 14:40:03,349 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:40:03,357 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=3 socket_options=None
2025-05-16 14:40:05,441 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F122B9B80>
2025-05-16 14:40:05,441 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-05-16 14:40:05,442 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:40:05,443 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-05-16 14:40:05,444 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:40:05,444 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-05-16 14:40:05,458 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:40:05 GMT'), (b'server', b'uvicorn'), (b'content-length', b'16875'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-16 14:40:05,458 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:40:05,459 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-05-16 14:40:05,459 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:40:05,459 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:40:05,459 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:40:05,459 - httpcore.connection - DEBUG - close.started
2025-05-16 14:40:05,460 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:40:05,462 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:40:05,529 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/11" 200 0
2025-05-16 14:40:15,159 - matplotlib - DEBUG - matplotlib data path: C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\matplotlib\mpl-data
2025-05-16 14:40:15,163 - matplotlib - DEBUG - CONFIGDIR=C:\Users\lovel\.matplotlib
2025-05-16 14:40:15,165 - matplotlib - DEBUG - interactive is False
2025-05-16 14:40:15,165 - matplotlib - DEBUG - platform is win32
2025-05-16 14:40:15,220 - matplotlib - DEBUG - CACHEDIR=C:\Users\lovel\.matplotlib
2025-05-16 14:40:15,223 - matplotlib.font_manager - DEBUG - Using fontManager instance from C:\Users\lovel\.matplotlib\fontlist-v390.json
2025-05-16 14:40:15,410 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:40:15,410 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:40:15,411 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:40:15,419 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:40:15,421 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:40:15,482 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:40:15,483 - __main__ - DEBUG - Processing message: hello
2025-05-16 14:40:15,483 - __main__ - DEBUG - Using backend: gemini for input: hello
2025-05-16 14:40:16,193 - __main__ - DEBUG - Generated response: Hello! How can I help you with calculations today?...
2025-05-16 14:40:16,194 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:40:18,508 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:40:18,508 - __main__ - INFO - Switched to openai backend
2025-05-16 14:40:18,510 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:40:23,619 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:40:23,621 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:40:23,626 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:40:23,627 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:40:23,676 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:40:23,678 - __main__ - DEBUG - Processing message: can you be the best?
2025-05-16 14:40:23,678 - __main__ - DEBUG - Using backend: openai for input: can you be the best?
2025-05-16 14:40:23,678 - __main__ - DEBUG - Sending to OpenAI: [{'role': 'system', 'content': 'You are a helpful assistant that can perform calculations and answer questions concisely.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'Hello! How can I help you with calculations today?'}, {'role': 'user', 'content': 'can you be the best?'}]
2025-05-16 14:40:23,686 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant that can perform calculations and answer questions concisely.'}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'Hello! How can I help you with calculations today?'}, {'role': 'user', 'content': 'can you be the best?'}], 'model': 'o4-mini-2025-04-16', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:40:23,739 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:40:23,739 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:40:23,769 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F12CF78F0>
2025-05-16 14:40:23,769 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000022F0FD8BED0> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:40:23,787 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000022F12CF7650>
2025-05-16 14:40:23,788 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:40:23,788 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:40:23,788 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:40:23,788 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:40:23,789 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:40:23,946 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 400, b'Bad Request', [(b'Date', b'Fri, 16 May 2025 18:40:24 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'245'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'48'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'80'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'199953'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'13ms'), (b'x-request-id', b'req_a5e422fee7cef1559282e7a6a8709e27'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=aotWmXBYv4LzFE3nW8iHOxX6wUUbtJwCcBFQSbjPqbc-1747420824-1.0.1.1-8WKTKwpZbNTGYZddGJyT4MuVTYypLNfM0kAhRrK8x2rdoeo6vVQMAMTYn23v7x5gRJpHK4w6n_Zq_tEaFQ0Gu90RMJhktnOXnjKyCiYJkI8; path=/; expires=Fri, 16-May-25 19:10:24 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=H.6qyVCnN3OQA6WeCU6Uyg106u2yxRzIZ.UUComqDHI-1747420824088-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940d00d5abf41891-EWR'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:40:23,947 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-05-16 14:40:23,947 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:40:23,947 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:40:23,948 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:40:23,948 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:40:23,948 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "400 Bad Request" Headers([('date', 'Fri, 16 May 2025 18:40:24 GMT'), ('content-type', 'application/json'), ('content-length', '245'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-xilu0sqkeveasxfwmh1ozfxa'), ('openai-processing-ms', '48'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '80'), ('x-ratelimit-limit-requests', '500'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '499'), ('x-ratelimit-remaining-tokens', '199953'), ('x-ratelimit-reset-requests', '120ms'), ('x-ratelimit-reset-tokens', '13ms'), ('x-request-id', 'req_a5e422fee7cef1559282e7a6a8709e27'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=aotWmXBYv4LzFE3nW8iHOxX6wUUbtJwCcBFQSbjPqbc-1747420824-1.0.1.1-8WKTKwpZbNTGYZddGJyT4MuVTYypLNfM0kAhRrK8x2rdoeo6vVQMAMTYn23v7x5gRJpHK4w6n_Zq_tEaFQ0Gu90RMJhktnOXnjKyCiYJkI8; path=/; expires=Fri, 16-May-25 19:10:24 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=H.6qyVCnN3OQA6WeCU6Uyg106u2yxRzIZ.UUComqDHI-1747420824088-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '940d00d5abf41891-EWR'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-05-16 14:40:23,948 - openai._base_client - DEBUG - request_id: req_a5e422fee7cef1559282e7a6a8709e27
2025-05-16 14:40:23,948 - openai._base_client - DEBUG - Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1043, in _request
    response.raise_for_status()
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.openai.com/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400
2025-05-16 14:40:23,949 - openai._base_client - DEBUG - Not retrying
2025-05-16 14:40:23,949 - openai._base_client - DEBUG - Re-raising status error
2025-05-16 14:40:23,950 - __main__ - ERROR - OpenAI API Error: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 153, in openai_chat_response
    response = openai_client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_utils\_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\resources\chat\completions.py", line 863, in create
    return self._post(
           ^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1283, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 960, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 1064, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'type': 'invalid_request_error', 'param': 'max_tokens', 'code': 'unsupported_parameter'}}
2025-05-16 14:40:23,951 - __main__ - DEBUG - Generated response: I'm sorry, I encountered an error with OpenAI: Error code: 400 - {'error': {'message': "Unsupported parameter: 'max_tokens' is not supported with this model. Use 'max_completion_tokens' instead.", 'ty...
2025-05-16 14:40:23,952 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:41:16,228 - __main__ - DEBUG - OpenAI client initialized successfully
2025-05-16 14:41:16,972 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:41:16,972 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:41:16,973 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:41:16,973 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:41:16,973 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:41:21,102 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002CD29541D60>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:41:21,102 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:41:21,106 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:41:21,278 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:41:21,417 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:41:21,436 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:41:21,469 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:41:21,469 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:41:21,473 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2ADB2CC0>
2025-05-16 14:41:21,473 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002CD2996B350> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:41:21,493 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2AD16D80>
2025-05-16 14:41:21,493 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002CD29AB8850> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:41:21,498 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2ADB2BD0>
2025-05-16 14:41:21,499 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:41:21,499 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:41:21,499 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:41:21,499 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:41:21,499 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:41:21,516 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2ADB39E0>
2025-05-16 14:41:21,516 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:41:21,517 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:41:21,517 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:41:21,517 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:41:21,517 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:41:21,518 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:41:21 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:41:21,518 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:41:21,518 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:41:21,518 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:41:21,519 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:41:21,519 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:41:21,519 - httpcore.connection - DEBUG - close.started
2025-05-16 14:41:21,519 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:41:21,521 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:41:21,537 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2ACBA300>
2025-05-16 14:41:21,537 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002CD2996B6D0> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:41:21,539 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:41:21 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:41:21,539 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:41:21,539 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:41:21,539 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:41:21,539 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:41:21,540 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:41:21,540 - httpcore.connection - DEBUG - close.started
2025-05-16 14:41:21,540 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:41:21,541 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:41:21,556 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=None socket_options=None
2025-05-16 14:41:21,583 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2ADB3410>
2025-05-16 14:41:21,583 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002CD29AB8A50> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:41:21,594 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:41:21,606 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:41:21,729 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD29AE5550>
2025-05-16 14:41:21,730 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:41:21,730 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:41:21,730 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:41:21,730 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:41:21,730 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:41:21,776 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2AC6B020>
2025-05-16 14:41:21,776 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:41:21,776 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:41:21,777 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:41:21,777 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:41:21,777 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:41:21,825 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:41:21 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:41:21,826 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:41:21,826 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:41:21,826 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:41:21,826 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:41:21,826 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:41:21,826 - httpcore.connection - DEBUG - close.started
2025-05-16 14:41:21,827 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:41:21,873 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:41:21 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:41:21,874 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:41:21,874 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:41:21,874 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:41:21,874 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:41:21,875 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:41:21,875 - httpcore.connection - DEBUG - close.started
2025-05-16 14:41:21,875 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:41:23,592 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD29ABC2F0>
2025-05-16 14:41:23,593 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:41:23,593 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:41:23,594 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:41:23,595 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:41:23,595 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:41:23,596 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:41:23 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-05-16 14:41:23,596 - httpx - INFO - HTTP Request: GET http://localhost:7861/startup-events "HTTP/1.1 200 OK"
2025-05-16 14:41:23,596 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:41:23,596 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:41:23,596 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:41:23,597 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:41:23,597 - httpcore.connection - DEBUG - close.started
2025-05-16 14:41:23,597 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:41:23,605 - httpcore.connection - DEBUG - connect_tcp.started host='localhost' port=7861 local_address=None timeout=3 socket_options=None
2025-05-16 14:41:25,656 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2ADE53A0>
2025-05-16 14:41:25,657 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-05-16 14:41:25,657 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:41:25,658 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-05-16 14:41:25,658 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:41:25,658 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-05-16 14:41:25,669 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Fri, 16 May 2025 18:41:25 GMT'), (b'server', b'uvicorn'), (b'content-length', b'16875'), (b'content-type', b'text/html; charset=utf-8')])
2025-05-16 14:41:25,669 - httpx - INFO - HTTP Request: HEAD http://localhost:7861/ "HTTP/1.1 200 OK"
2025-05-16 14:41:25,669 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-05-16 14:41:25,669 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:41:25,671 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:41:25,671 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:41:25,671 - httpcore.connection - DEBUG - close.started
2025-05-16 14:41:25,672 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:41:25,674 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:41:25,738 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/11" 200 0
2025-05-16 14:41:40,627 - matplotlib - DEBUG - matplotlib data path: C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\matplotlib\mpl-data
2025-05-16 14:41:40,632 - matplotlib - DEBUG - CONFIGDIR=C:\Users\lovel\.matplotlib
2025-05-16 14:41:40,633 - matplotlib - DEBUG - interactive is False
2025-05-16 14:41:40,633 - matplotlib - DEBUG - platform is win32
2025-05-16 14:41:40,688 - matplotlib - DEBUG - CACHEDIR=C:\Users\lovel\.matplotlib
2025-05-16 14:41:40,690 - matplotlib.font_manager - DEBUG - Using fontManager instance from C:\Users\lovel\.matplotlib\fontlist-v390.json
2025-05-16 14:41:40,879 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:41:40,879 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:41:40,880 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:41:40,886 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:41:40,887 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:41:40,950 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:41:40,952 - __main__ - DEBUG - Processing message: afga
2025-05-16 14:41:40,953 - __main__ - DEBUG - Using backend: gemini for input: afga
2025-05-16 14:41:41,941 - __main__ - DEBUG - Generated response: Okay, I am ready to perform calculations and provide exact numerical results....
2025-05-16 14:41:41,941 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:41:45,308 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:41:45,309 - __main__ - INFO - Switched to openai backend
2025-05-16 14:41:45,310 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:41:47,511 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:41:47,513 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:41:47,526 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:41:47,526 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:41:47,563 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:41:47,565 - __main__ - DEBUG - Processing message: and you?
2025-05-16 14:41:47,565 - __main__ - DEBUG - Using backend: openai for input: and you?
2025-05-16 14:41:47,565 - __main__ - DEBUG - Sending to OpenAI: [{'role': 'system', 'content': 'You are a helpful assistant that can perform calculations and answer questions concisely.'}, {'role': 'user', 'content': 'afga'}, {'role': 'assistant', 'content': 'Okay, I am ready to perform calculations and provide exact numerical results.'}, {'role': 'user', 'content': 'and you?'}]
2025-05-16 14:41:47,569 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant that can perform calculations and answer questions concisely.'}, {'role': 'user', 'content': 'afga'}, {'role': 'assistant', 'content': 'Okay, I am ready to perform calculations and provide exact numerical results.'}, {'role': 'user', 'content': 'and you?'}], 'model': 'gpt-4.1-mini-2025-04-14', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:41:47,638 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:41:47,639 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:41:47,735 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2BA3E270>
2025-05-16 14:41:47,736 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002CD288BBED0> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:41:47,774 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2B9FDD90>
2025-05-16 14:41:47,774 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:41:47,775 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:41:47,775 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:41:47,775 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:41:47,775 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:41:48,671 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:41:48 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'790'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'795'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'199950'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'15ms'), (b'x-request-id', b'req_61511b47f57e9de7f0661e2e65cb383f'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=xi5EucbN2If7L046Qytt1ztWGoAsdrowUbR98LZaYOo-1747420908-1.0.1.1-TyOD9nGZI5SJtRNnK80SBTss4gcZcFS7vaXLBti2elArDKkFvuugOHmLXg3lXGdR81YGaRvvJUxX2b0.iZb433.D3XsdYzsCIUaqgF6G0Cw; path=/; expires=Fri, 16-May-25 19:11:48 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=B3_FdL9FP.ZD35DxdBX4Zbhts8tbv3fhDNU2waTeA.Q-1747420908803-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940d02e29bc6e991-ORD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:41:48,672 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 14:41:48,672 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:41:48,672 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:41:48,673 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:41:48,673 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:41:48,673 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers([('date', 'Fri, 16 May 2025 18:41:48 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-organization', 'user-xilu0sqkeveasxfwmh1ozfxa'), ('openai-processing-ms', '790'), ('openai-version', '2020-10-01'), ('x-envoy-upstream-service-time', '795'), ('x-ratelimit-limit-requests', '500'), ('x-ratelimit-limit-tokens', '200000'), ('x-ratelimit-remaining-requests', '499'), ('x-ratelimit-remaining-tokens', '199950'), ('x-ratelimit-reset-requests', '120ms'), ('x-ratelimit-reset-tokens', '15ms'), ('x-request-id', 'req_61511b47f57e9de7f0661e2e65cb383f'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=xi5EucbN2If7L046Qytt1ztWGoAsdrowUbR98LZaYOo-1747420908-1.0.1.1-TyOD9nGZI5SJtRNnK80SBTss4gcZcFS7vaXLBti2elArDKkFvuugOHmLXg3lXGdR81YGaRvvJUxX2b0.iZb433.D3XsdYzsCIUaqgF6G0Cw; path=/; expires=Fri, 16-May-25 19:11:48 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=B3_FdL9FP.ZD35DxdBX4Zbhts8tbv3fhDNU2waTeA.Q-1747420908803-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '940d02e29bc6e991-ORD'), ('content-encoding', 'br'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-05-16 14:41:48,673 - openai._base_client - DEBUG - request_id: req_61511b47f57e9de7f0661e2e65cb383f
2025-05-16 14:41:48,676 - __main__ - DEBUG - Received from OpenAI: Im here to help with calculations and provide precise numerical answers. How can I assist you today?
2025-05-16 14:41:48,676 - __main__ - DEBUG - Generated response: Im here to help with calculations and provide precise numerical answers. How can I assist you today?...
2025-05-16 14:41:48,676 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:42:03,091 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:42:03,093 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:42:03,100 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:42:03,101 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:42:03,139 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:42:03,140 - __main__ - DEBUG - Processing message: what are you best at?
2025-05-16 14:42:03,141 - __main__ - DEBUG - Using backend: openai for input: what are you best at?
2025-05-16 14:42:03,141 - __main__ - DEBUG - Sending to OpenAI: [{'role': 'system', 'content': 'You are a helpful assistant that can perform calculations and answer questions concisely.'}, {'role': 'user', 'content': 'afga'}, {'role': 'assistant', 'content': 'Okay, I am ready to perform calculations and provide exact numerical results.'}, {'role': 'user', 'content': 'and you?'}, {'role': 'assistant', 'content': 'Im here to help with calculations and provide precise numerical answers. How can I assist you today?'}, {'role': 'user', 'content': 'what are you best at?'}]
2025-05-16 14:42:03,145 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant that can perform calculations and answer questions concisely.'}, {'role': 'user', 'content': 'afga'}, {'role': 'assistant', 'content': 'Okay, I am ready to perform calculations and provide exact numerical results.'}, {'role': 'user', 'content': 'and you?'}, {'role': 'assistant', 'content': 'Im here to help with calculations and provide precise numerical answers. How can I assist you today?'}, {'role': 'user', 'content': 'what are you best at?'}], 'model': 'gpt-4.1-mini-2025-04-14', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:42:03,146 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:42:03,146 - httpcore.connection - DEBUG - close.started
2025-05-16 14:42:03,147 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:42:03,147 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:42:03,185 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2ADB3800>
2025-05-16 14:42:03,186 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002CD288BBED0> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:42:03,217 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2B687800>
2025-05-16 14:42:03,217 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:42:03,219 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:42:03,219 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:42:03,219 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:42:03,219 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:42:04,446 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:42:04 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'1128'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1132'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'199917'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'24ms'), (b'x-request-id', b'req_827df24b982c29bc79d450250670d6f5'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940d03432d6b0046-ORD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:42:04,447 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 14:42:04,447 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:42:04,452 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:42:04,452 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:42:04,452 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:42:04,453 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 16 May 2025 18:42:04 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-xilu0sqkeveasxfwmh1ozfxa', 'openai-processing-ms': '1128', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1132', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '199917', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '24ms', 'x-request-id': 'req_827df24b982c29bc79d450250670d6f5', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '940d03432d6b0046-ORD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
2025-05-16 14:42:04,453 - openai._base_client - DEBUG - request_id: req_827df24b982c29bc79d450250670d6f5
2025-05-16 14:42:04,454 - __main__ - DEBUG - Received from OpenAI: I excel at performing calculations, solving math problems, and providing precise numerical results. If you have any math-related questions or need assistance with numbers, feel free to ask!
2025-05-16 14:42:04,454 - __main__ - DEBUG - Generated response: I excel at performing calculations, solving math problems, and providing precise numerical results. If you have any math-related questions or need assistance with numbers, feel free to ask!...
2025-05-16 14:42:04,455 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:47:12,536 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:47:12,537 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:47:12,542 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:47:12,543 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:47:12,580 - matplotlib.pyplot - DEBUG - Loaded backend agg version v2.2.
2025-05-16 14:47:12,583 - __main__ - DEBUG - Processing message: what is the sun?
2025-05-16 14:47:12,583 - __main__ - DEBUG - Using backend: openai for input: what is the sun?
2025-05-16 14:47:12,583 - __main__ - DEBUG - Sending to OpenAI: [{'role': 'system', 'content': 'You are a helpful assistant that can perform calculations and answer questions concisely.'}, {'role': 'user', 'content': 'afga'}, {'role': 'assistant', 'content': 'Okay, I am ready to perform calculations and provide exact numerical results.'}, {'role': 'user', 'content': 'and you?'}, {'role': 'assistant', 'content': 'Im here to help with calculations and provide precise numerical answers. How can I assist you today?'}, {'role': 'user', 'content': 'what are you best at?'}, {'role': 'assistant', 'content': 'I excel at performing calculations, solving math problems, and providing precise numerical results. If you have any math-related questions or need assistance with numbers, feel free to ask!'}, {'role': 'user', 'content': 'what is the sun?'}]
2025-05-16 14:47:12,588 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are a helpful assistant that can perform calculations and answer questions concisely.'}, {'role': 'user', 'content': 'afga'}, {'role': 'assistant', 'content': 'Okay, I am ready to perform calculations and provide exact numerical results.'}, {'role': 'user', 'content': 'and you?'}, {'role': 'assistant', 'content': 'Im here to help with calculations and provide precise numerical answers. How can I assist you today?'}, {'role': 'user', 'content': 'what are you best at?'}, {'role': 'assistant', 'content': 'I excel at performing calculations, solving math problems, and providing precise numerical results. If you have any math-related questions or need assistance with numbers, feel free to ask!'}, {'role': 'user', 'content': 'what is the sun?'}], 'model': 'gpt-4.1-mini-2025-04-14', 'max_tokens': 150, 'temperature': 0.7}}
2025-05-16 14:47:12,588 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-05-16 14:47:12,589 - httpcore.connection - DEBUG - close.started
2025-05-16 14:47:12,589 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:47:12,590 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-05-16 14:47:12,667 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2BA601A0>
2025-05-16 14:47:12,667 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002CD288BBED0> server_hostname='api.openai.com' timeout=5.0
2025-05-16 14:47:12,709 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002CD2BA697F0>
2025-05-16 14:47:12,709 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-05-16 14:47:12,709 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:47:12,709 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-05-16 14:47:12,709 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:47:12,709 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-05-16 14:47:13,932 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:47:14 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-organization', b'user-xilu0sqkeveasxfwmh1ozfxa'), (b'openai-processing-ms', b'1005'), (b'openai-version', b'2020-10-01'), (b'x-envoy-upstream-service-time', b'1011'), (b'x-ratelimit-limit-requests', b'500'), (b'x-ratelimit-limit-tokens', b'200000'), (b'x-ratelimit-remaining-requests', b'499'), (b'x-ratelimit-remaining-tokens', b'199863'), (b'x-ratelimit-reset-requests', b'120ms'), (b'x-ratelimit-reset-tokens', b'40ms'), (b'x-request-id', b'req_bbb52691aa757dae8d510965e753004b'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'940d0ad178da22e6-ORD'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-05-16 14:47:13,932 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-16 14:47:13,933 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-05-16 14:47:13,936 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:47:13,936 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:47:13,936 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:47:13,937 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "200 OK" Headers({'date': 'Fri, 16 May 2025 18:47:14 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-expose-headers': 'X-Request-ID', 'openai-organization': 'user-xilu0sqkeveasxfwmh1ozfxa', 'openai-processing-ms': '1005', 'openai-version': '2020-10-01', 'x-envoy-upstream-service-time': '1011', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '200000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '199863', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '40ms', 'x-request-id': 'req_bbb52691aa757dae8d510965e753004b', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '940d0ad178da22e6-ORD', 'content-encoding': 'br', 'alt-svc': 'h3=":443"; ma=86400'})
2025-05-16 14:47:13,937 - openai._base_client - DEBUG - request_id: req_bbb52691aa757dae8d510965e753004b
2025-05-16 14:47:13,937 - __main__ - DEBUG - Received from OpenAI: The Sun is a star at the center of our solar system. It is a nearly perfect sphere of hot plasma, primarily composed of hydrogen and helium, and it produces energy through nuclear fusion, which powers life on Earth.
2025-05-16 14:47:13,938 - __main__ - DEBUG - Generated response: The Sun is a star at the center of our solar system. It is a nearly perfect sphere of hot plasma, primarily composed of hydrogen and helium, and it produces energy through nuclear fusion, which powers...
2025-05-16 14:47:13,938 - matplotlib.pyplot - DEBUG - Loaded backend tkagg version 8.6.
2025-05-16 14:53:33,655 - __main__ - DEBUG - OpenAI client initialized successfully
2025-05-16 14:53:34,437 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:53:34,437 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:53:34,438 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:53:34,438 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:53:34,439 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:53:38,528 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BE3C9F3320>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:53:38,529 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:53:38,535 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:53:38,569 - __main__ - ERROR - Failed to start application: List.__init__() got an unexpected keyword argument 'container'
2025-05-16 14:53:38,769 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:53:38,770 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:53:38,813 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001BE0BF6C1D0>
2025-05-16 14:53:38,813 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001BE3C9FF850> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:53:38,835 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001BE0BF6C110>
2025-05-16 14:53:38,836 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:53:38,836 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:53:38,836 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:53:38,837 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:53:38,837 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:53:38,858 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:53:39 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:53:38,858 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:53:38,858 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:53:38,859 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:53:38,859 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:53:38,859 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:53:38,859 - httpcore.connection - DEBUG - close.started
2025-05-16 14:53:38,859 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:53:38,860 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:53:38,869 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001BE0BF6C980>
2025-05-16 14:53:38,869 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001BE3C9FF350> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:53:38,940 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:53:39,062 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001BE3C9715E0>
2025-05-16 14:53:39,063 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:53:39,063 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:53:39,063 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:53:39,063 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:53:39,063 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:53:39,159 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:53:39 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:53:39,159 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:53:39,160 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:53:39,160 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:53:39,160 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:53:39,160 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:53:39,161 - httpcore.connection - DEBUG - close.started
2025-05-16 14:53:39,161 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:54:31,848 - __main__ - DEBUG - OpenAI client initialized successfully
2025-05-16 14:54:32,586 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:54:32,586 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:54:32,586 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:54:32,586 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:54:32,587 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:54:36,670 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020113E63200>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:54:36,670 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:54:36,675 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:54:36,712 - __main__ - ERROR - Failed to start application: List.__init__() got an unexpected keyword argument 'container'
2025-05-16 14:54:36,934 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:54:36,937 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:54:36,983 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002016333C230>
2025-05-16 14:54:36,983 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020113E6F750> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:54:37,007 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002016333C170>
2025-05-16 14:54:37,008 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:54:37,008 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:54:37,008 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:54:37,008 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:54:37,008 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:54:37,029 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:54:37 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:54:37,029 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:54:37,029 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:54:37,030 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:54:37,030 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:54:37,030 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:54:37,030 - httpcore.connection - DEBUG - close.started
2025-05-16 14:54:37,030 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:54:37,031 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:54:37,051 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002016333C9E0>
2025-05-16 14:54:37,051 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020113E6F350> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:54:37,121 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:54:37,247 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020113D830E0>
2025-05-16 14:54:37,247 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:54:37,247 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:54:37,248 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:54:37,248 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:54:37,248 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:54:37,346 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:54:37 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:54:37,347 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:54:37,347 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:54:37,347 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:54:37,348 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:54:37,348 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:54:37,348 - httpcore.connection - DEBUG - close.started
2025-05-16 14:54:37,348 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:55:26,645 - __main__ - DEBUG - OpenAI client initialized successfully
2025-05-16 14:55:27,378 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:55:27,378 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:55:27,378 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:55:27,379 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:55:27,379 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:55:31,468 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000026DE01A0E30>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:55:31,468 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:55:31,473 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:55:31,505 - __main__ - ERROR - Failed to start application: List.__init__() got an unexpected keyword argument 'container'
2025-05-16 14:55:31,702 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:55:31,703 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:55:31,744 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026DAFB5C230>
2025-05-16 14:55:31,744 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026DE05BF750> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:55:31,768 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026DE065F860>
2025-05-16 14:55:31,768 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:55:31,768 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:55:31,769 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:55:31,769 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:55:31,769 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:55:31,790 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:55:31 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:55:31,790 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:55:31,790 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:55:31,791 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:55:31,791 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:55:31,791 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:55:31,791 - httpcore.connection - DEBUG - close.started
2025-05-16 14:55:31,791 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:55:31,792 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:55:31,800 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026DAFB5C980>
2025-05-16 14:55:31,800 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000026DE05BF350> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:55:31,878 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:55:31,994 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000026DDFEB20F0>
2025-05-16 14:55:31,995 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:55:31,995 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:55:31,996 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:55:31,996 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:55:31,996 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:55:32,090 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:55:32 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:55:32,090 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:55:32,091 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:55:32,091 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:55:32,091 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:55:32,091 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:55:32,091 - httpcore.connection - DEBUG - close.started
2025-05-16 14:55:32,092 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:57:21,817 - __main__ - DEBUG - OpenAI client initialized successfully
2025-05-16 14:57:22,558 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:57:22,558 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:57:22,558 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:57:22,560 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:57:22,561 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): localhost:6789
2025-05-16 14:57:26,656 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001CAEA7F30B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:57:26,657 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:57:26,664 - asyncio - DEBUG - Using proactor: IocpProactor
2025-05-16 14:57:26,699 - __main__ - ERROR - Failed to start application: List.__init__() got an unexpected keyword argument 'container'
2025-05-16 14:57:26,893 - httpcore.connection - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:57:26,896 - httpcore.connection - DEBUG - connect_tcp.started host='checkip.amazonaws.com' port=443 local_address=None timeout=3 socket_options=None
2025-05-16 14:57:26,949 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CAB9CDC950>
2025-05-16 14:57:26,950 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001CAEA7FF750> server_hostname='checkip.amazonaws.com' timeout=3
2025-05-16 14:57:26,972 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CAB9CDC830>
2025-05-16 14:57:26,973 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:57:26,973 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:57:26,974 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:57:26,974 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:57:26,974 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:57:26,988 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CAEA776330>
2025-05-16 14:57:26,988 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001CAEA7FF450> server_hostname='api.gradio.app' timeout=3
2025-05-16 14:57:26,996 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'', [(b'Date', b'Fri, 16 May 2025 18:57:27 GMT'), (b'Content-Type', b'text/plain;charset=UTF-8'), (b'Content-Length', b'13'), (b'Connection', b'keep-alive'), (b'Server', b'nginx'), (b'Vary', b'Origin'), (b'Vary', b'Access-Control-Request-Method'), (b'Vary', b'Access-Control-Request-Headers')])
2025-05-16 14:57:26,996 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:57:26,997 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:57:26,997 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:57:26,997 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:57:26,997 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:57:26,998 - httpcore.connection - DEBUG - close.started
2025-05-16 14:57:26,998 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:57:26,999 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-05-16 14:57:27,126 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/11" 200 0
2025-05-16 14:57:27,188 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001CAEA865CA0>
2025-05-16 14:57:27,189 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-05-16 14:57:27,189 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-05-16 14:57:27,189 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-05-16 14:57:27,189 - httpcore.http11 - DEBUG - send_request_body.complete
2025-05-16 14:57:27,189 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-05-16 14:57:27,286 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 16 May 2025 18:57:27 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-05-16 14:57:27,287 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 14:57:27,287 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-05-16 14:57:27,287 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-05-16 14:57:27,287 - httpcore.http11 - DEBUG - response_closed.started
2025-05-16 14:57:27,289 - httpcore.http11 - DEBUG - response_closed.complete
2025-05-16 14:57:27,289 - httpcore.connection - DEBUG - close.started
2025-05-16 14:57:27,289 - httpcore.connection - DEBUG - close.complete
2025-05-16 14:59:28,215 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 14:59:28,216 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 14:59:28,216 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 14:59:28,216 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 14:59:32,308 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B50DB62F30>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 14:59:32,309 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 14:59:32,346 - __main__ - ERROR - Failed to start application: List.__init__() got an unexpected keyword argument 'container'
2025-05-16 14:59:32,639 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 14:59:32,936 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:00:09,877 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:00:09,877 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:00:09,877 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 15:00:09,877 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:00:13,993 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001F294949AC0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:00:13,994 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:00:14,032 - __main__ - ERROR - Failed to start application: List.__init__() got an unexpected keyword argument 'container'
2025-05-16 15:00:14,337 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:00:14,635 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:00:31,215 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:00:31,216 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:00:31,216 - __main__ - INFO - Visit http://0.0.0.0:7861 to chat
2025-05-16 15:00:31,216 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:00:35,320 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024D3DF9F350>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:00:35,321 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:00:35,674 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:00:35,695 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:00:35,989 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:02:17,234 - __main__ - INFO - ==================================================
2025-05-16 15:02:17,235 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:02:17,235 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:02:17,235 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:02:17,235 - __main__ - INFO - ==================================================
2025-05-16 15:02:17,235 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:02:21,366 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000017261525280>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:02:21,366 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:02:21,722 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:02:21,742 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:02:22,049 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:02:33,051 - __main__ - INFO - ==================================================
2025-05-16 15:02:33,051 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:02:33,051 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:02:33,052 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:02:33,052 - __main__ - INFO - ==================================================
2025-05-16 15:02:33,052 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:02:37,139 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B370417CE0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:02:37,139 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:02:37,497 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:02:37,563 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:02:37,935 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:02:49,181 - __main__ - INFO - ==================================================
2025-05-16 15:02:49,182 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:02:49,182 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:02:49,182 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:02:49,182 - __main__ - INFO - ==================================================
2025-05-16 15:02:49,182 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:02:53,282 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000271A2433B00>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:02:53,282 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:02:53,716 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:02:53,723 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:02:54,029 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:03:05,200 - __main__ - INFO - ==================================================
2025-05-16 15:03:05,200 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:03:05,201 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:03:05,201 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:03:05,201 - __main__ - INFO - ==================================================
2025-05-16 15:03:05,201 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:03:09,316 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001A85DC5BB60>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:03:09,317 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:03:09,710 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:03:09,727 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:03:10,011 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:03:21,184 - __main__ - INFO - ==================================================
2025-05-16 15:03:21,185 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:03:21,185 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:03:21,185 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:03:21,185 - __main__ - INFO - ==================================================
2025-05-16 15:03:21,185 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:03:25,281 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001A8F6457B90>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:03:25,282 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:03:25,675 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:03:25,679 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:03:25,914 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:03:37,218 - __main__ - INFO - ==================================================
2025-05-16 15:03:37,219 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:03:37,219 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:03:37,219 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:03:37,219 - __main__ - INFO - ==================================================
2025-05-16 15:03:37,219 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:03:41,331 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D5486101A0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:03:41,331 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:03:41,688 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:03:41,716 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:03:42,000 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:03:53,476 - __main__ - INFO - ==================================================
2025-05-16 15:03:53,476 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:03:53,476 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:03:53,477 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:03:53,477 - __main__ - INFO - ==================================================
2025-05-16 15:03:53,477 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:03:57,573 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000174B2644B00>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:03:57,574 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:03:57,940 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:03:57,956 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:03:58,244 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:04:09,524 - __main__ - INFO - ==================================================
2025-05-16 15:04:09,525 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:04:09,525 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:04:09,525 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:04:09,525 - __main__ - INFO - ==================================================
2025-05-16 15:04:09,525 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:04:13,641 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001CDEA057C80>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:04:13,642 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:04:14,020 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:04:14,023 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:04:14,341 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:04:25,446 - __main__ - INFO - ==================================================
2025-05-16 15:04:25,446 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:04:25,446 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:04:25,446 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:04:25,448 - __main__ - INFO - ==================================================
2025-05-16 15:04:25,448 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:04:29,554 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020CA7F12F00>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:04:29,554 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:04:29,942 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:04:29,961 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:04:30,240 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:04:41,452 - __main__ - INFO - ==================================================
2025-05-16 15:04:41,452 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:04:41,452 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:04:41,453 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:04:41,453 - __main__ - INFO - ==================================================
2025-05-16 15:04:41,453 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:04:45,553 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000013E18404680>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:04:45,553 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:04:45,920 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:04:45,939 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:04:46,219 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:04:57,450 - __main__ - INFO - ==================================================
2025-05-16 15:04:57,451 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:04:57,451 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:04:57,451 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:04:57,451 - __main__ - INFO - ==================================================
2025-05-16 15:04:57,451 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:05:01,572 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D16AA14BC0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:05:01,572 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:05:01,944 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:05:01,963 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:05:02,256 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:05:13,342 - __main__ - INFO - ==================================================
2025-05-16 15:05:13,342 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:05:13,342 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:05:13,342 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:05:13,342 - __main__ - INFO - ==================================================
2025-05-16 15:05:13,342 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:05:17,446 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001B3615A4C20>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:05:17,447 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:05:17,834 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:05:17,850 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:05:18,064 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:05:39,436 - __main__ - INFO - ==================================================
2025-05-16 15:05:39,437 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:05:39,437 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:05:39,437 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:05:39,437 - __main__ - INFO - ==================================================
2025-05-16 15:05:39,437 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:05:43,555 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BF6C94DA00>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:05:43,555 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:05:43,916 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:05:43,935 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:05:44,227 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:05:55,385 - __main__ - INFO - ==================================================
2025-05-16 15:05:55,386 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:05:55,386 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:05:55,386 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:05:55,386 - __main__ - INFO - ==================================================
2025-05-16 15:05:55,386 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:05:59,516 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002771F59D7F0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:05:59,516 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:05:59,892 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:05:59,896 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:06:00,208 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:06:11,423 - __main__ - INFO - ==================================================
2025-05-16 15:06:11,424 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:06:11,424 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:06:11,424 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:06:11,424 - __main__ - INFO - ==================================================
2025-05-16 15:06:11,424 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:06:15,525 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002128DE9D910>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:06:15,525 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:06:15,918 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:06:15,933 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:06:16,156 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:06:27,383 - __main__ - INFO - ==================================================
2025-05-16 15:06:27,384 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:06:27,384 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:06:27,384 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:06:27,384 - __main__ - INFO - ==================================================
2025-05-16 15:06:27,384 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:06:31,504 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002C6B8F7FEC0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:06:31,504 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:06:31,879 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:06:31,898 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:06:32,173 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:06:43,444 - __main__ - INFO - ==================================================
2025-05-16 15:06:43,445 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:06:43,445 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:06:43,445 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:06:43,445 - __main__ - INFO - ==================================================
2025-05-16 15:06:43,446 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:06:47,553 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001E54F0217C0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:06:47,553 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:06:47,927 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:06:47,932 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:06:48,234 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:06:59,583 - __main__ - INFO - ==================================================
2025-05-16 15:06:59,583 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:06:59,583 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:06:59,583 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:06:59,583 - __main__ - INFO - ==================================================
2025-05-16 15:06:59,583 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:07:03,698 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001F41513E030>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:07:03,699 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:07:04,063 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:07:04,078 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:07:04,384 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:07:15,400 - __main__ - INFO - ==================================================
2025-05-16 15:07:15,401 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:07:15,401 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:07:15,401 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:07:15,401 - __main__ - INFO - ==================================================
2025-05-16 15:07:15,401 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:07:19,491 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002050C7431D0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:07:19,492 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:07:19,864 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:07:19,867 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:07:20,166 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:07:31,425 - __main__ - INFO - ==================================================
2025-05-16 15:07:31,426 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:07:31,426 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:07:31,426 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:07:31,426 - __main__ - INFO - ==================================================
2025-05-16 15:07:31,426 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:07:35,525 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000242048DA180>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:07:35,525 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:07:35,896 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:07:35,922 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-16 15:07:36,217 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:10:36,017 - __main__ - INFO - ==================================================
2025-05-16 15:10:36,018 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:10:36,018 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:10:36,018 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:10:36,018 - __main__ - INFO - ==================================================
2025-05-16 15:10:36,019 - __main__ - INFO - Testing connection to MCP server...
2025-05-16 15:10:40,151 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001BFBBEDF3E0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:10:40,151 - __main__ - WARNING - Please make sure server.py is running in another terminal
2025-05-16 15:10:40,395 - numexpr.utils - INFO - Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-05-16 15:10:40,395 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.
2025-05-16 15:10:40,813 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 15:10:40,883 - __main__ - ERROR - Failed to start application: name 'os' is not defined
2025-05-16 15:17:22,410 - __main__ - INFO - ==================================================
2025-05-16 15:17:22,411 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 15:17:22,411 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 15:17:22,411 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 15:17:22,411 - __main__ - INFO - ==================================================
2025-05-16 15:17:22,411 - __main__ - INFO - Testing connection to MCP server (attempt 1/3)...
2025-05-16 15:17:26,512 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AC6A5040B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:17:26,513 - __main__ - WARNING - Retrying in 2 seconds...
2025-05-16 15:17:28,514 - __main__ - INFO - Testing connection to MCP server (attempt 2/3)...
2025-05-16 15:17:32,631 - __main__ - WARNING - Could not connect to MCP server: HTTPConnectionPool(host='localhost', port=6789): Max retries exceeded with url: /greeting/Test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001AC6A5047A0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-16 15:17:32,633 - __main__ - WARNING - Retrying in 4 seconds...
2025-05-16 15:17:36,634 - __main__ - INFO - Testing connection to MCP server (attempt 3/3)...
2025-05-16 15:17:40,719 - __main__ - ERROR - Failed to connect to MCP server after 3 attempts
2025-05-16 15:17:40,719 - __main__ - ERROR - Please make sure server.py is running in another terminal
2025-05-16 15:17:40,721 - __main__ - ERROR - Command to start server: python server.py --port 6789
2025-05-16 15:17:44,632 - __main__ - INFO - Exiting...
2025-05-16 16:24:13,756 - __main__ - INFO - ==================================================
2025-05-16 16:24:13,757 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 16:24:13,757 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 16:24:13,757 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 16:24:13,757 - __main__ - INFO - ==================================================
2025-05-16 16:24:13,758 - __main__ - INFO - Testing connection to MCP server (attempt 1/3)...
2025-05-16 16:24:15,830 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-16 16:24:16,067 - numexpr.utils - INFO - Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-05-16 16:24:16,067 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.
2025-05-16 16:24:16,412 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 16:24:16,551 - __main__ - ERROR - Failed to start application: module 'gradio' has no attribute 'Variable'
2025-05-16 16:38:10,936 - __main__ - INFO - ==================================================
2025-05-16 16:38:10,938 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 16:38:10,938 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 16:38:10,939 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 16:38:10,939 - __main__ - INFO - ==================================================
2025-05-16 16:38:10,940 - __main__ - INFO - Testing connection to MCP server (attempt 1/3)...
2025-05-16 16:38:13,021 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-16 16:38:13,251 - numexpr.utils - INFO - Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-05-16 16:38:13,251 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.
2025-05-16 16:38:13,644 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 16:38:13,705 - __main__ - ERROR - Failed to start application: EventListener._setup.<locals>.event_trigger() got an unexpected keyword argument 'prevent_thread_lock'
2025-05-16 17:27:02,754 - __main__ - INFO - ==================================================
2025-05-16 17:27:02,754 - __main__ - INFO - Starting MCP Chatbot...
2025-05-16 17:27:02,754 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-16 17:27:02,755 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-16 17:27:02,755 - __main__ - INFO - ==================================================
2025-05-16 17:27:02,755 - __main__ - INFO - Testing connection to MCP server (attempt 1/3)...
2025-05-16 17:27:04,806 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-16 17:27:05,050 - numexpr.utils - INFO - Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-05-16 17:27:05,050 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.
2025-05-16 17:27:05,386 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-16 17:27:05,565 - __main__ - ERROR - Failed to start application: EventListener._setup.<locals>.event_trigger() got an unexpected keyword argument 'prevent_thread_lock'
2025-05-18 08:30:01,816 - __main__ - WARNING - Google Generative AI package not installed. Install with: pip install google-generativeai
2025-05-18 08:30:03,860 - __main__ - INFO - ==================================================
2025-05-18 08:30:03,860 - __main__ - INFO - Starting MCP Chatbot...
2025-05-18 08:30:03,860 - __main__ - INFO - MCP Server URL: http://localhost:6789
2025-05-18 08:30:03,860 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-18 08:30:03,860 - __main__ - INFO - ==================================================
2025-05-18 08:30:03,860 - __main__ - INFO - Testing connection to MCP server (attempt 1/3)...
2025-05-18 08:30:05,923 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-18 08:30:06,617 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 08:30:07,632 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-18 08:30:07,835 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ "HTTP/1.1 200 OK"
2025-05-18 08:34:50,160 - __main__ - INFO - ==================================================
2025-05-18 08:34:50,161 - __main__ - INFO - Starting MCP Chatbot...
2025-05-18 08:34:50,161 - __main__ - INFO - MCP Server URL: http://localhost:6790
2025-05-18 08:34:50,163 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-18 08:34:50,163 - __main__ - INFO - ==================================================
2025-05-18 08:34:50,164 - __main__ - INFO - Testing connection to MCP server (attempt 1/3)...
2025-05-18 08:34:52,239 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-18 08:34:52,858 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 08:34:53,629 - numexpr.utils - INFO - Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-05-18 08:34:53,630 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.
2025-05-18 08:34:55,217 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-18 08:34:55,233 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ "HTTP/1.1 200 OK"
2025-05-18 08:35:20,114 - __main__ - INFO - Switched to openai backend
2025-05-18 08:35:24,106 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-18 08:35:24,409 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-18 08:41:44,737 - __main__ - INFO - ==================================================
2025-05-18 08:41:44,737 - __main__ - INFO - Starting MCP Chatbot...
2025-05-18 08:41:44,738 - __main__ - INFO - MCP Server URL: http://localhost:6790
2025-05-18 08:41:44,738 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-18 08:41:44,738 - __main__ - INFO - ==================================================
2025-05-18 08:41:44,738 - __main__ - INFO - Testing connection to MCP server (attempt 1/3)...
2025-05-18 08:41:46,811 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-18 08:41:47,071 - numexpr.utils - INFO - Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-05-18 08:41:47,071 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.
2025-05-18 08:41:47,406 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 08:41:48,178 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-18 08:41:48,217 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ "HTTP/1.1 200 OK"
2025-05-18 08:47:34,925 - __main__ - INFO - ==================================================
2025-05-18 08:47:34,926 - __main__ - INFO - Starting MCP Chatbot...
2025-05-18 08:47:34,926 - __main__ - INFO - MCP Server URL: http://localhost:6790
2025-05-18 08:47:34,927 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-18 08:47:34,927 - __main__ - INFO - ==================================================
2025-05-18 08:47:34,927 - __main__ - INFO - Testing connection to MCP server (attempt 1/3)...
2025-05-18 08:47:36,970 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-18 08:47:37,195 - numexpr.utils - INFO - Note: NumExpr detected 20 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-05-18 08:47:37,195 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.
2025-05-18 08:47:37,613 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 08:47:38,064 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-05-18 08:47:38,091 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ "HTTP/1.1 200 OK"
2025-05-18 08:48:15,190 - __main__ - INFO - Switched to openai backend
2025-05-18 08:48:19,740 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-18 08:48:20,170 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-05-18 08:52:17,550 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 57, in <module>
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_client.py", line 112, in __init__
    super().__init__(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 793, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
                                  ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 08:52:17,551 - __main__ - WARNING - OpenAI functionality will be disabled
2025-05-18 08:52:20,295 - __main__ - INFO - ==================================================
2025-05-18 08:52:20,295 - __main__ - INFO - Starting MCP Chatbot...
2025-05-18 08:52:20,295 - __main__ - INFO - MCP Server URL: http://localhost:6790
2025-05-18 08:52:20,295 - __main__ - INFO - Web Interface: http://localhost:7861
2025-05-18 08:52:20,296 - __main__ - INFO - ==================================================
2025-05-18 08:52:20,296 - __main__ - INFO - Testing connection to MCP server (attempt 1/3)...
2025-05-18 08:52:22,348 - __main__ - INFO - Server test successful: 200 - {"result":"Hello, Test! Nice to meet you."}
2025-05-18 08:52:22,380 - __main__ - ERROR - Failed to start application: Chatbot.__init__() got an unexpected keyword argument 'placeholder'
2025-05-18 08:52:22,682 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 08:52:22,963 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 08:52:23,313 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:03:42,507 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 57, in <module>
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_client.py", line 112, in __init__
    super().__init__(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 793, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
                                  ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 09:03:42,508 - __main__ - WARNING - OpenAI functionality will be disabled
2025-05-18 09:03:43,103 - __main__ - ERROR - Failed to start application: name 'load_dotenv' is not defined
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 567, in main
    load_dotenv()
    ^^^^^^^^^^^
NameError: name 'load_dotenv' is not defined
2025-05-18 09:04:11,107 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 58, in <module>
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_client.py", line 112, in __init__
    super().__init__(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 793, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
                                  ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 09:04:11,108 - __main__ - WARNING - OpenAI functionality will be disabled
2025-05-18 09:04:11,707 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 09:04:11,708 - __main__ - WARNING - OpenAI features will be disabled
2025-05-18 09:04:11,708 - __main__ - INFO - Gemini client initialized successfully
2025-05-18 09:04:15,821 - __main__ - WARNING - Could not connect to MCP server at http://localhost:8000: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001C218ECB650>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-18 09:04:15,854 - __main__ - ERROR - Failed to start application: Chatbot.__init__() got an unexpected keyword argument 'placeholder'
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 617, in main
    chatbot = gr.Chatbot(
              ^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\gradio\component_meta.py", line 159, in wrapper
    return fn(self, **kwargs)
           ^^^^^^^^^^^^^^^^^^
TypeError: Chatbot.__init__() got an unexpected keyword argument 'placeholder'
2025-05-18 09:04:16,164 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:04:16,452 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:04:16,797 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:04:35,338 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 58, in <module>
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_client.py", line 112, in __init__
    super().__init__(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 793, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
                                  ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 09:04:35,339 - __main__ - WARNING - OpenAI functionality will be disabled
2025-05-18 09:04:36,029 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 09:04:36,029 - __main__ - WARNING - OpenAI features will be disabled
2025-05-18 09:04:36,029 - __main__ - INFO - Gemini client initialized successfully
2025-05-18 09:04:40,098 - __main__ - WARNING - Could not connect to MCP server at http://localhost:8000: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001E93E253560>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-18 09:04:40,399 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:04:40,737 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:04:41,083 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:04:42,533 - httpx - INFO - HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-18 09:04:44,753 - httpx - INFO - HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-18 09:04:45,449 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ "HTTP/1.1 200 OK"
2025-05-18 09:07:49,116 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 58, in <module>
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_client.py", line 112, in __init__
    super().__init__(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 793, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
                                  ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 09:07:49,119 - __main__ - WARNING - OpenAI functionality will be disabled
2025-05-18 09:07:49,748 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 09:07:49,748 - __main__ - WARNING - OpenAI features will be disabled
2025-05-18 09:07:49,748 - __main__ - INFO - Gemini client initialized successfully
2025-05-18 09:07:53,847 - __main__ - WARNING - Could not connect to MCP server at http://localhost:8000: HTTPConnectionPool(host='localhost', port=8000): Max retries exceeded with url: /health (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000267AEA23C50>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-18 09:07:54,172 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:07:54,497 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:07:54,812 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:07:58,983 - __main__ - ERROR - Failed to start application: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 709, in main
    demo.launch(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\gradio\blocks.py", line 1998, in launch
    ) = networking.start_server(
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\gradio\networking.py", line 207, in start_server
    raise OSError(
OSError: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.
2025-05-18 09:13:48,700 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 58, in <module>
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_client.py", line 112, in __init__
    super().__init__(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 793, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
                                  ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 09:13:48,703 - __main__ - WARNING - OpenAI functionality will be disabled
2025-05-18 09:13:49,659 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:13:49,948 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:13:50,310 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:16:04,936 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 58, in <module>
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_client.py", line 112, in __init__
    super().__init__(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 793, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
                                  ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 09:16:04,937 - __main__ - WARNING - OpenAI functionality will be disabled
2025-05-18 09:16:05,847 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:16:06,150 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:16:06,504 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:16:35,356 - __main__ - ERROR - Failed to initialize OpenAI client: 'Session' object has no attribute 'timeout'
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 58, in <module>
    openai_client = OpenAI(api_key=OPENAI_API_KEY,
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_client.py", line 112, in __init__
    super().__init__(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 775, in __init__
    if http_client and http_client.timeout != HTTPX_DEFAULT_TIMEOUT:
                       ^^^^^^^^^^^^^^^^^^^
AttributeError: 'Session' object has no attribute 'timeout'
2025-05-18 09:16:35,358 - __main__ - WARNING - OpenAI functionality will be disabled
2025-05-18 09:16:36,271 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:16:36,581 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:16:36,901 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:16:54,808 - __main__ - ERROR - Failed to initialize OpenAI client: Client.__init__() got an unexpected keyword argument 'proxies'
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 59, in <module>
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_client.py", line 112, in __init__
    super().__init__(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\openai\_base_client.py", line 793, in __init__
    self._client = http_client or SyncHttpxClientWrapper(
                                  ^^^^^^^^^^^^^^^^^^^^^^^
TypeError: Client.__init__() got an unexpected keyword argument 'proxies'
2025-05-18 09:16:54,809 - __main__ - WARNING - OpenAI functionality will be disabled
2025-05-18 09:16:55,723 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:16:56,043 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:16:56,349 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:18:09,609 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:18:10,028 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:18:10,349 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:18:28,340 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:18:28,656 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:18:28,975 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:21:11,984 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:21:12,287 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:21:12,730 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:22:27,761 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:22:28,062 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:22:28,388 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:22:55,883 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:22:56,217 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:22:56,508 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:23:12,381 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:23:12,690 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:23:13,023 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:23:14,169 - __main__ - INFO - Connected to MCP server at http://localhost:6790
2025-05-18 09:23:14,783 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:23:14,872 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:23:16,500 - httpx - INFO - HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-18 09:23:18,576 - httpx - INFO - HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-18 09:23:19,210 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ "HTTP/1.1 200 OK"
2025-05-18 09:25:48,393 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:25:48,677 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:25:49,047 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:25:50,176 - __main__ - INFO - Connected to MCP server at http://localhost:6790
2025-05-18 09:25:50,787 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:25:50,897 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:25:55,245 - __main__ - ERROR - Failed to start application: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 800, in main
    demo.launch(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\gradio\blocks.py", line 1998, in launch
    ) = networking.start_server(
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\gradio\networking.py", line 207, in start_server
    raise OSError(
OSError: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.
2025-05-18 09:26:09,679 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:26:09,980 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:26:10,315 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:26:13,547 - __main__ - WARNING - Could not connect to MCP server at http://localhost:6790: HTTPConnectionPool(host='localhost', port=6790): Max retries exceeded with url: /tool/add?a=2&b=3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000028C4DE9D580>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-18 09:26:14,169 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:26:14,269 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:26:15,898 - httpx - INFO - HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-18 09:26:17,959 - httpx - INFO - HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-18 09:26:18,640 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ "HTTP/1.1 200 OK"
2025-05-18 09:29:16,332 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:29:16,625 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:29:16,988 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:29:20,149 - __main__ - WARNING - Could not connect to MCP server at http://localhost:6790: HTTPConnectionPool(host='localhost', port=6790): Max retries exceeded with url: /tool/add?a=2&b=3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025D4A470E60>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-18 09:29:20,727 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:29:20,841 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:29:25,215 - __main__ - ERROR - Failed to start application: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.
Traceback (most recent call last):
  File "C:\Users\lovel\source\repos\gevans3000\MCP Agent Context\mcp_chatbot\app.py", line 827, in main
    demo.launch(
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\gradio\blocks.py", line 1998, in launch
    ) = networking.start_server(
        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lovel\AppData\Roaming\Python\Python312\site-packages\gradio\networking.py", line 207, in start_server
    raise OSError(
OSError: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.
2025-05-18 09:29:33,006 - httpx - INFO - HTTP Request: GET https://checkip.amazonaws.com/ "HTTP/1.1 200 "
2025-05-18 09:29:33,327 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:29:33,646 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:29:36,866 - __main__ - WARNING - Could not connect to MCP server at http://localhost:6790: HTTPConnectionPool(host='localhost', port=6790): Max retries exceeded with url: /tool/add?a=2&b=3 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000179F8F43530>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))
2025-05-18 09:29:37,491 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-05-18 09:29:37,566 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ "HTTP/1.1 200 OK"
2025-05-18 09:29:39,253 - httpx - INFO - HTTP Request: GET http://localhost:7860/startup-events "HTTP/1.1 200 OK"
2025-05-18 09:29:41,310 - httpx - INFO - HTTP Request: HEAD http://localhost:7860/ "HTTP/1.1 200 OK"
2025-05-18 09:29:41,959 - httpx - INFO - HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ "HTTP/1.1 200 OK"
